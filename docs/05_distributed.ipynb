{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f167c0-52a8-4800-9999-0419acc312a8",
   "metadata": {},
   "source": [
    "# Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae8044-47c9-4c48-b50d-e3dadb441757",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukeconibear/intro_ml/blob/main/docs/05_distributed.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb78754a-ddf7-4ccd-a1a2-18a055f97adc",
   "metadata": {},
   "source": [
    "```{note}\n",
    "If youâ€™re in COLAB or have a local CUDA GPU, you can follow along with the more computationally intensive training in this lesson.\n",
    "\n",
    "For those in COLAB, ensure the session is using a GPU by going to: Runtime > Change runtime type > Hardware accelerator = GPU.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aef6fc-d7fe-45a5-9593-f9092ae02ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're using colab, then install the required modules\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install --quiet --upgrade ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8aa72b-89d2-47e8-9dae-d6eff282617a",
   "metadata": {},
   "source": [
    "Distributing training over multiple devices generally uses either:\n",
    "\n",
    "- [Data parallelism](https://developers.google.com/machine-learning/glossary/#data-parallelism)\n",
    "    - _Split the **data** over multiple devices._\n",
    "    - Single model copied to multiple devices.\n",
    "    - Useful for big data.\n",
    "    - Simpler.\n",
    "- [Model parallelism](https://developers.google.com/machine-learning/glossary/#model-parallelism)\n",
    "    - _Split the **model** over multiple devices._\n",
    "    - Single data copied to multiple devices.\n",
    "    - Can be useful for big models (for some architectures).\n",
    "    - More complex.\n",
    "    \n",
    "This lesson focuses on data parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f972d1-8eac-424a-b800-19a6c1f6ef04",
   "metadata": {},
   "source": [
    "## [Ray Train](https://docs.ray.io/en/latest/train/train.html)\n",
    "\n",
    "Ray Train is a useful tool for distributed deep learning training for [TensorFlow (Keras)](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) and [PyTorch](https://pytorch.org/tutorials/beginner/dist_overview.html).\n",
    "\n",
    "It handles the set up for you (e.g., [`TF_CONFIG`](https://www.tensorflow.org/guide/distributed_training#setting_up_the_tf_config_environment_variable) in TensorFlow).\n",
    "\n",
    "There are a range of examples [here](https://docs.ray.io/en/latest/train/examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03524baa-bf2a-4817-a0df-08b03c2d2142",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Note, Ray doesn't currently work on POWER9 machines e.g., Bede. See, [GitHub issue](https://github.com/ray-project/ray/issues/7476).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864cbb8-4483-4baa-99ea-ae49a58fa6db",
   "metadata": {},
   "source": [
    "### Example: [TensorFlow (Keras) MNIST](https://docs.ray.io/en/latest/train/examples/tensorflow_mnist_example.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401135c-c7be-4ef6-83ce-4476dc9dd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "from ray.train import Trainer\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340dd1ed-38e0-455c-9f2b-08445bf302a4",
   "metadata": {},
   "source": [
    "#### [Define callback for reporting](https://docs.ray.io/en/latest/train/user_guide.html#logging-monitoring-and-callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe996158-18c1-4d96-8d6a-f57fd9a46110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainReportCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ray.train.report(**logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e6947-55dd-4e48-a550-4213b4e16bc9",
   "metadata": {},
   "source": [
    "#### Set up the dataset and model\n",
    "\n",
    "The dataset will be split (sharded) across the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef5e61-5392-4edd-bdaf-fec50b2752cf",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "The default [auto-sharding](https://www.tensorflow.org/tutorials/distribute/input#sharding) by `FILE` can cause warning messages if the data is in one file. Instead, you can specify to auto-shard by data using: `tf.data.experimental.AutoShardPolicy.DATA` (which it will fall back to anyway).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1ba8c-03a4-434e-9409-8ff0e17246e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(batch_size):\n",
    "    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "    # The `x` arrays are in uint8 and have values in the [0, 255] range.\n",
    "    # You need to convert them to float32 with values in the [0, 1] range.\n",
    "    x_train = x_train / np.float32(255)\n",
    "    y_train = y_train.astype(np.int64)\n",
    "    ds_train = (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        .shuffle(60000)\n",
    "        .repeat()\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = (\n",
    "        tf.data.experimental.AutoShardPolicy.DATA\n",
    "    )\n",
    "    ds_train = ds_train.with_options(options)\n",
    "\n",
    "    return ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c32824f-b4a9-46fd-b802-a2cccad58ecf",
   "metadata": {},
   "source": [
    "When building the model, a `config` keyword argument is added to get different options such as the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc516c-9738-4397-97ae-309a568564e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_cnn_model(config):\n",
    "    learning_rate = config.get(\"lr\", 0.001)\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(28, 28)),\n",
    "            tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61991c2f-cf4b-4ef2-943d-bf22e3d40ba2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set up the training function for a _single_ worker\n",
    "\n",
    "This also uses the `config` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3ca5a-badd-49b6-8cff-5b8c29c62c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    batch_size = 64\n",
    "    single_worker_dataset = mnist_dataset(batch_size)\n",
    "    single_worker_model = build_and_compile_cnn_model(config)\n",
    "    single_worker_model.fit(\n",
    "        single_worker_dataset,\n",
    "        epochs=config[\"epochs\"],\n",
    "        steps_per_epoch=70,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd0d13-143e-4c86-918e-5d28a1361a43",
   "metadata": {},
   "source": [
    "Now, you [configure training](https://docs.ray.io/en/latest/train/user_guide.html#configuring-training) using the `config` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f7875-5432-4ac1-ab40-240b72d95310",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"epochs\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb09cc-1b1a-4884-9d4f-026c0115816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_func(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea930e9-6620-4e67-903a-52c3f76da168",
   "metadata": {},
   "source": [
    "#### [Update training function](https://docs.ray.io/en/latest/train/user_guide.html#update-training-function)\n",
    "\n",
    "1. Set the _global_ batch size\n",
    "    - Each worker will process the same size batch as in the single-worker code.\n",
    "2. Choose your TensorFlow distributed training strategy.\n",
    "    - In this example we use the [MultiWorkerMirroredStrategy](https://www.tensorflow.org/guide/distributed_training#multiworkermirroredstrategy) for synchronous training of multiple workers across many machines.\n",
    "        - For multiple workers on _one_ machine, use [MirroredStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy).\n",
    "        - In general, the mirrored strategy mirrors the parameters across the workers, ensuring replicas are identical.\n",
    "    - Within the strategy scope context manager, you build and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8604122-4b78-4459-a49e-0716854003fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    per_worker_batch_size = config.get(\"batch_size\", 64)\n",
    "    epochs = config.get(\"epochs\", 3)\n",
    "    steps_per_epoch = config.get(\"steps_per_epoch\", 70)\n",
    "\n",
    "    tf_config = json.loads(os.environ[\"TF_CONFIG\"])\n",
    "    num_workers = len(tf_config[\"cluster\"][\"worker\"])\n",
    "\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "    global_batch_size = per_worker_batch_size * num_workers\n",
    "    multi_worker_dataset = mnist_dataset(global_batch_size)\n",
    "\n",
    "    with strategy.scope():\n",
    "        # model building/compiling need to be within strategy.scope()\n",
    "        multi_worker_model = build_and_compile_cnn_model(config)\n",
    "\n",
    "    history = multi_worker_model.fit(\n",
    "        multi_worker_dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=[TrainReportCallback()],\n",
    "        verbose=False,\n",
    "    )\n",
    "    results = history.history\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fab04c-ae80-415d-9f41-de20b03b65ae",
   "metadata": {},
   "source": [
    "#### [Create Ray Train Trainer](https://docs.ray.io/en/latest/train/user_guide.html#create-ray-train-trainer)\n",
    "\n",
    "The `Trainer` manages state and training.\n",
    "\n",
    "Ray Train enables different `backend` options e.g., `tensorflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919384c-3a06-4cc5-a733-e98257473ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tensorflow_mnist(num_workers=1, use_gpu=False, epochs=4):\n",
    "    trainer = Trainer(backend=\"tensorflow\", num_workers=num_workers, use_gpu=use_gpu)\n",
    "    trainer.start()\n",
    "    results = trainer.run(\n",
    "        train_func=train_func, config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": epochs}\n",
    "    )\n",
    "    trainer.shutdown()\n",
    "    print(f\"Results: {results[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6ad38-736d-40d1-9025-655f838509a5",
   "metadata": {},
   "source": [
    "#### [Run the training](https://docs.ray.io/en/latest/train/user_guide.html#run-training-function)\n",
    "\n",
    "Initialise and shutdown the Ray client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c478acf-9517-4cef-91a5-d26506e7b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fce682-d5af-495b-b2da-bed0feefefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu\n",
    "# train_tensorflow_mnist()\n",
    "\n",
    "# gpu\n",
    "# train_tensorflow_mnist(use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c511b19-86e0-4a82-8e6e-48f10f51d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d5b43-b647-484a-ae72-95ddb3ab0961",
   "metadata": {},
   "source": [
    "#### Submit the job to HPC\n",
    "\n",
    "This Python script is in full [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/tensorflow_ray_train_mnist_example.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31ed3a-e683-4990-ab93-ed849418a360",
   "metadata": {},
   "source": [
    "##### CPUs\n",
    "\n",
    "An example job submission script is (also [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/distributed_ml_on_arc4_cpu.bash)):\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#$ -cwd -V\n",
    "#$ -l h_rt=00:30:00\n",
    "#$ -pe smp 12\n",
    "#$ -l h_vmem=6G\n",
    "\n",
    "# activate conda and add to library path\n",
    "conda activate intro_ml\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib\n",
    "\n",
    "# run the CPU script\n",
    "python tensorflow_ray_train_mnist_example.py --num-workers 12 --epochs 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e7e19-e14e-4ef8-bad0-e7c28b6038e8",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Sometimes the `LD_LIBRARY_PATH` variable will not include the path to the conda environment, resulting it in being unable to find some libraries e.g., TensorFlow, cuDNN.\n",
    "\n",
    "To resolve this, in your script append the path to your activated conda environment to the `LD_LIBRARY_PATH` variable using:\n",
    "\n",
    "`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib`  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdaf4d0-53b1-4cbd-b2b0-637cc052f9f4",
   "metadata": {},
   "source": [
    "```{note}\n",
    "These scripts are based on a personal miniconda with the environment activated at the terminal and the terminal `LD_LIBRARY_PATH` updated.  \n",
    "\n",
    "There are other options e.g., using the [Anaconda](https://arcdocs.leeds.ac.uk/software/compilers/anaconda.html) distribution installed on ARC.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe98ff-87b6-4765-a753-6d71898e2c40",
   "metadata": {},
   "source": [
    "To change the number of workers (either CPUs or GPUs), set the `--num-workers` argument.\n",
    "\n",
    "Ensure that the scheduler request matches this number e.g.,:\n",
    "\n",
    "- For 12 CPU workers, use `--num-workers 12` and `#$ -pe smp 12`.\n",
    "- For 2 GPU workers, use `--num-workers 2` and `#$ -l coproc_v100=2`.\n",
    "    - To use GPUs, you will also need the `--use-gpu True` flag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae10ba3-b47a-4974-beb7-d947eec175a0",
   "metadata": {},
   "source": [
    "In this simple example using 12 CPUs, the job efficiency was (using `qacct -j <JOBID>`):\n",
    "\n",
    "```\n",
    "Efficiency = 100 * cpu / (ru_wallclock * slots)\n",
    "Efficiency = 100 * 10214 / (928 * 12)\n",
    "Efficiency = 92 %\n",
    "```\n",
    "\n",
    "92% is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312974e-d089-426d-8e88-8fa47ed4cb10",
   "metadata": {},
   "source": [
    "##### GPU(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fe073-473e-4ce0-9b2c-012b4dc3dcf6",
   "metadata": {},
   "source": [
    "```{note}\n",
    "The tests below were for a [NVIDIA V100 with 32 GB of memory](https://arcdocs.leeds.ac.uk/usage/gpgpu.html#arc4).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7f591-53aa-4861-aea5-b14994d58d9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "An example job submission script is (also [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/distributed_ml_on_arc4_gpu.bash)):\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#$ -cwd -V\n",
    "#$ -l h_rt=00:15:00\n",
    "#$ -l coproc_v100=1\n",
    "\n",
    "# activate conda and add to library path\n",
    "conda activate swd8_intro_ml \n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib\n",
    "\n",
    "# start the efficiency log for the GPU\n",
    "nvidia-smi dmon -d 10 -s um -i 0 > efficiency_log &\n",
    "\n",
    "# run the GPU script\n",
    "python tensorflow_ray_train_mnist_example.py --use-gpu True --num-workers 1 --epochs 100\n",
    "\n",
    "# stop the efficiency log\n",
    "kill %1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796288c-7942-488e-8bd3-ce0a740caef4",
   "metadata": {},
   "source": [
    "You can check the efficiency of your GPU job using the [NVIDIA System Management Interface](https://developer.nvidia.com/nvidia-system-management-interface) i.e., `nvidia-smi`.\n",
    "\n",
    "- `dmon` is for device monitoring.\n",
    "- `-d X` records the metrics every X seconds.\n",
    "- `-s um` is the metrics to sample, where u = utilisation and m = memory.\n",
    "- `-i 0` is the device number, where 0 = first gpu. For multiple devices, add them separated by commas e.g., `-i 0,1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51124d83-4885-407c-b996-5f30a90373ef",
   "metadata": {},
   "source": [
    "Then you can view the results from the `efficiency_log`:\n",
    "\n",
    "```bash\n",
    "# gpu    sm   mem   enc   dec    fb  bar1\n",
    "# Idx     %     %     %     %    MB    MB\n",
    "    0     0     0     0     0     0     2\n",
    "    0     0     0     0     0     0     2\n",
    "    0     0     0     0     0 31880     5\n",
    "    0    34    15     0     0 31932     5\n",
    "```\n",
    "\n",
    "The main columns are `sm` (for utilisation %) and `mem` (for memory %).\n",
    "\n",
    "Here, the job utilised approximately 35% of the GPU and 15% of its memory. (The first few rows are blank as the GPU was idle while the CPU downloaded the data.)\n",
    "\n",
    "That's quite low, though this was for a simple MNIST example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86feb54f-8008-437d-b219-4a413f926655",
   "metadata": {},
   "source": [
    "A more complex problem will likely utilise the GPU better.\n",
    "\n",
    "### Example: [TensorFlow (Keras) transfer learning](tf_transfer_learning)\n",
    "\n",
    "In lesson 4, we looked at a TensorFlow (Keras) transfer learning example. To use this script with Ray Train, you primarily need to create the dataset pipeline and model creation functions. You can use the examples here as a foundation. The full script for this example is [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/tensorflow_ray_train_transfer_learning_example.py).\n",
    "\n",
    "Running this code on a single GPU utilised approximately 90% of the GPU and 45% of its memory.\n",
    "\n",
    "This is good.\n",
    "\n",
    "Multiple GPUs is not needed here, though they could be requested by simply changing:  \n",
    "\n",
    "- Ray Train flag `--num-workers 2`\n",
    "- Scheduler option `#$ -l coproc_v100=2`\n",
    "- GPU monitoring `nvidia-smi dmon -i 0,1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1bac5-4b28-45aa-b9e3-706346d8b09d",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "To utilise GPU(s) efficiently, try to match the resources requested to what your problem requires e.g., the model of GPU, the memory it has, etc.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf4c2d-1031-47df-a076-863634cf360d",
   "metadata": {},
   "source": [
    "## [PyTorch (Lightning)](https://pytorch-lightning.readthedocs.io/en/stable/clouds/cluster.html)\n",
    "\n",
    "PyTorch Lightning comes with great built-in functionality to scale to multiple GPUs.\n",
    "\n",
    "First though, we should check that the problem requires these resources.\n",
    "\n",
    "A [simple example for MNIST](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/pytorch_lightning_mnist_example.py) utilises about 8% of a single V100 GPU.\n",
    "\n",
    "The [PyTorch Lightning transfer learning example](torch_transfer_learning) (lesson 4) utilises about 15% of a single V100 GPU (full script [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/pytorch_lightning_transfer_learning_example.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f0efa-844c-4ff8-9bc2-04d60d48e16f",
   "metadata": {},
   "source": [
    "To demonstrate running this transfer learning example over multiple GPUs, we could unfreeze the pre-trained model to increase the workload.\n",
    "\n",
    "This is normally not recommended, though is useful for demonstration purposes here of a complex model.\n",
    "\n",
    "To unfreeze the pre-trained model:\n",
    "\n",
    "- Set `trainer.finetune(strategy=\"unfreeze\")`.\n",
    "- This is also the default setting.\n",
    "\n",
    "Then the steps to run over multiple GPUs are:\n",
    "\n",
    "1. Specify the [data parallel strategy](https://pytorch-lightning.readthedocs.io/en/stable/advanced/model_parallel.html#ddp-optimizations) in the Trainer:\n",
    "\n",
    "    ```python\n",
    "    from pytorch_lightning.strategies import DDPStrategy\n",
    "\n",
    "    trainer = flash.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=torch.cuda.device_count(),\n",
    "        strategy=DDPStrategy(find_unused_parameters=False),\n",
    "    )\n",
    "    ```\n",
    "\n",
    "    - Note, setting `find_unused_parameters=False` [can improve performance](https://pytorch.org/docs/stable/notes/ddp.html#internal-design).\n",
    "\n",
    "\n",
    "2. Set the scheduler to use multiple GPUs:\n",
    "\n",
    "    - e.g., `#$ -l coproc_v100=2`.\n",
    "    \n",
    "\n",
    "\n",
    "3. Adding GPU monitoring for multiple GPUs:\n",
    "\n",
    "    - e.g., `nvidia-smi dmon -i 0,1`.\n",
    "\n",
    "\n",
    "This utilised 2 V100 GPUs at approximately 40%. This isn't great, though it demonstrates the ease of distributed training with PyTorch Lightning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044acf3f-b994-4fea-b45a-ed3af02013cc",
   "metadata": {},
   "source": [
    "## Jupyter Notebook to HPC\n",
    "\n",
    "Once you've finished testing out different ideas locally in a Jupyter Notebook, you can then convert this to an executable script to run on a HPC.\n",
    "\n",
    "This is because the HPCs (currently accessible at Leeds, at least) are suitable for non-interactive batch jobs.\n",
    "\n",
    "The steps are to:\n",
    "\n",
    "- [Clean non-essential code](clean_nonessential_code)\n",
    "- [Refactor Jupyter Notebook code into functions](refactor_into_functions)\n",
    "- [Create a Python script](create_python_script)\n",
    "- [Create submission script](create_submission_script)\n",
    "- [(Optional) Create unit tests](create_unit_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c2064-5b2a-4e68-88fe-dad5c41e6c3d",
   "metadata": {},
   "source": [
    "(clean_nonessential_code)=\n",
    "### Clean non-essential code\n",
    "\n",
    "Some code added during the experimentation phase was only needed to test out ideas and explore the data.\n",
    "\n",
    "This non-essential code can be removed to make it more maintainable and performant.\n",
    "\n",
    "Let's use the example from the [TensorFlow Datasets MNIST example](tensorflow_datasets) we saw in Lesson 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db364bea-8762-4079-b9df-00ca02ae2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# global setup\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# download the data\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "print(ds_train)\n",
    "print(ds_info)\n",
    "\n",
    "# preprocess the data\n",
    "def normalise_image(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "# create data pipelines\n",
    "def training_pipeline(ds_train):\n",
    "    ds_train = ds_train.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "    return ds_train\n",
    "\n",
    "\n",
    "def test_pipeline(ds_test):\n",
    "    ds_test = ds_test.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(AUTOTUNE)\n",
    "    return ds_test\n",
    "\n",
    "\n",
    "ds_train = training_pipeline(ds_train)\n",
    "ds_val = training_pipeline(ds_val)\n",
    "ds_test = test_pipeline(ds_test)\n",
    "\n",
    "\n",
    "# create the model\n",
    "inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "x = tf.keras.layers.Flatten(name=\"flatten\")(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer1\")(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer2\")(x)\n",
    "outputs = tf.keras.layers.Dense(10, name=\"outputs\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name=\"functional\")\n",
    "\n",
    "# view the model\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# plot the model accuracy\n",
    "epochs_range = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "plt.plot(epochs_range, history.history[\"accuracy\"], \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(\n",
    "    epochs_range, history.history[\"val_accuracy\"], \"b\", label=\"Validation accuracy\"\n",
    ")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0.9, 1.0])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# save the model\n",
    "path_models = f\"{os.getcwd()}/models\"\n",
    "model.save(f\"{path_models}/model_tf_mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99047dde-1025-424c-b918-a388f5d9d09f",
   "metadata": {},
   "source": [
    "We can now remove the non-essential code, as the experimentation phase is complete.\n",
    "\n",
    "Here, this included:\n",
    "\n",
    "- Removing data information from the download.\n",
    "- Removing unrequired print statements.\n",
    "- Viewing model summary.\n",
    "- Removing/replacing plots with text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a8b56-6eca-43a1-92d5-407a6d617099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# global setup\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# download the data\n",
    "(ds_train, ds_val, ds_test) = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=False,\n",
    ")\n",
    "\n",
    "# preprocess the data\n",
    "def normalise_image(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "# create data pipelines\n",
    "def training_pipeline(ds_train):\n",
    "    ds_train = ds_train.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "    return ds_train\n",
    "\n",
    "\n",
    "def test_pipeline(ds_test):\n",
    "    ds_test = ds_test.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(AUTOTUNE)\n",
    "    return ds_test\n",
    "\n",
    "\n",
    "ds_train = training_pipeline(ds_train)\n",
    "ds_val = training_pipeline(ds_val)\n",
    "ds_test = test_pipeline(ds_test)\n",
    "\n",
    "\n",
    "# create the model\n",
    "inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "x = tf.keras.layers.Flatten(name=\"flatten\")(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer1\")(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer2\")(x)\n",
    "outputs = tf.keras.layers.Dense(10, name=\"outputs\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name=\"functional\")\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# view the model accuracy\n",
    "print(f\"Training accuracy: {[round(num, 2) for num in history.history['accuracy']]}\")\n",
    "print(\n",
    "    f\"Validation accuracy: {[round(num, 2) for num in history.history['val_accuracy']]}\"\n",
    ")\n",
    "\n",
    "# save the model\n",
    "path_models = f\"{os.getcwd()}/models\"\n",
    "model.save(f\"{path_models}/model_tf_mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c735a7-9c23-4614-8fe1-23c007f63aaf",
   "metadata": {},
   "source": [
    "(refactor_into_functions)=\n",
    "### Refactor Jupyter Notebook code into functions\n",
    "\n",
    "Now, any code that is not already in functions, can be refactored into functions.\n",
    "\n",
    "Modularising the code like this helps with diagnosing errors and creating tests.\n",
    "\n",
    "Here, we added functions for:\n",
    "\n",
    "- Downloading the data.\n",
    "- Creating and compiling the model.\n",
    "- Training the model.\n",
    "- Saving the model.\n",
    "- A main function for the whole workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f1100-48d6-4d73-aa3f-805d15293c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# global setup\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# download the data\n",
    "def download_data():\n",
    "    (ds_train, ds_val, ds_test) = tfds.load(\n",
    "        \"mnist\",\n",
    "        split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=False,\n",
    "    )\n",
    "    return ds_train, ds_val, ds_test\n",
    "\n",
    "\n",
    "# preprocess the data\n",
    "def normalise_image(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "# create data pipelines\n",
    "def training_pipeline(ds_train):\n",
    "    ds_train = ds_train.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "    return ds_train\n",
    "\n",
    "\n",
    "def test_pipeline(ds_test):\n",
    "    ds_test = ds_test.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(AUTOTUNE)\n",
    "    return ds_test\n",
    "\n",
    "\n",
    "# create and compile the model\n",
    "def create_and_compile_model():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "    x = tf.keras.layers.Flatten(name=\"flatten\")(inputs)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer1\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer2\")(x)\n",
    "    outputs = tf.keras.layers.Dense(10, name=\"outputs\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"functional\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train_model():\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "# save the model\n",
    "def save_model(model):\n",
    "    path_models = f\"{os.getcwd()}/models\"\n",
    "    model.save(f\"{path_models}/model_tf_mnist\")\n",
    "\n",
    "\n",
    "# combine the functions in a call to main\n",
    "def main():\n",
    "    ds_train, ds_val, ds_test = download_data()\n",
    "\n",
    "    ds_train = training_pipeline(ds_train)\n",
    "    ds_val = training_pipeline(ds_val)\n",
    "    ds_test = test_pipeline(ds_test)\n",
    "\n",
    "    model = create_and_compile_model()\n",
    "    history = train_model()\n",
    "    save_model(model)\n",
    "\n",
    "\n",
    "# run the functions\n",
    "main()\n",
    "\n",
    "# view the model accuracy\n",
    "print(f\"Training accuracy: {[round(num, 2) for num in history.history['accuracy']]}\")\n",
    "print(\n",
    "    f\"Validation accuracy: {[round(num, 2) for num in history.history['val_accuracy']]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8197e4-a426-4629-8f3b-b67acb1cff6a",
   "metadata": {},
   "source": [
    "(create_python_script)=\n",
    "### Create a Python script\n",
    "\n",
    "First, the call to `main()` should be placed inside a conditional invocation i.e.,:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "Now, the script can be called from a terminal by running `python script.py`.\n",
    "\n",
    "Then, the rest of the code can be converted by either:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8e7db-e3c8-4411-930c-79d08bc5db2e",
   "metadata": {},
   "source": [
    "#### Option 1 (recommended)\n",
    "\n",
    "Convert a _single cell_ to a script using the `%%writefile` IPython magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0195d-937e-4847-bc35-a001a8718c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile jupyter-to-hpc_tf-mnist-example.py\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# global setup\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# download the data\n",
    "def download_data():\n",
    "    (ds_train, ds_val, ds_test) = tfds.load(\n",
    "        \"mnist\",\n",
    "        split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=False,\n",
    "    )\n",
    "    return ds_train, ds_val, ds_test\n",
    "\n",
    "\n",
    "# preprocess the data\n",
    "def normalise_image(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "# create data pipelines\n",
    "def training_pipeline(ds_train):\n",
    "    ds_train = ds_train.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "    return ds_train\n",
    "\n",
    "\n",
    "def test_pipeline(ds_test):\n",
    "    ds_test = ds_test.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(AUTOTUNE)\n",
    "    return ds_test\n",
    "\n",
    "\n",
    "# create and compile the model\n",
    "def create_and_compile_model():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "    x = tf.keras.layers.Flatten(name=\"flatten\")(inputs)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer1\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer2\")(x)\n",
    "    outputs = tf.keras.layers.Dense(10, name=\"outputs\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"functional\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train_model():\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "# save the model\n",
    "def save_model(model):\n",
    "    path_models = f\"{os.getcwd()}/models\"\n",
    "    model.save(f\"{path_models}/model_tf_mnist\")\n",
    "\n",
    "\n",
    "# combine the functions in a call to main\n",
    "def main():\n",
    "    ds_train, ds_val, ds_test = download_data()\n",
    "\n",
    "    ds_train = training_pipeline(ds_train)\n",
    "    ds_val = training_pipeline(ds_val)\n",
    "    ds_test = test_pipeline(ds_test)\n",
    "\n",
    "    model = create_and_compile_model()\n",
    "    history = train_model()\n",
    "    save_model(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run the functions\n",
    "    main()\n",
    "\n",
    "    # view the model accuracy\n",
    "    print(\n",
    "        f\"Training accuracy: {[round(num, 2) for num in history.history['accuracy']]}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Validation accuracy: {[round(num, 2) for num in history.history['val_accuracy']]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b5ebd-e3d2-4d72-87e5-f4cf2c9441e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Option 2\n",
    "\n",
    "Convert the _entire_ notebook to a script using the `nbconvert` package:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert \"my_notebook.ipynb\" --to script\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac845d9-4447-460f-b908-374b3391004f",
   "metadata": {},
   "source": [
    "#### Option 3\n",
    "\n",
    "Convert the _entire_ notebook to a script using the Jupyter menu.\n",
    "\n",
    "From within the Jupyter Notebook, click `File` > `Save and Export Notebook As ...` > `Executable Script`.\n",
    "\n",
    "This should convert the Jupyter Notebook to a `.py` file, and download it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e145615-080f-4d65-b25d-3b8b3daf00e4",
   "metadata": {},
   "source": [
    "(create_submission_script)=\n",
    "### Create submission script\n",
    "\n",
    "For your HPC (e.g., ARC4), create the submission script.\n",
    "\n",
    "For example:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#$ -cwd\n",
    "#$ -l h_rt=00:05:00\n",
    "#$ -l coproc_v100=1\n",
    "\n",
    "conda activate intro_ml \n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib  # (sometimes required)\n",
    "\n",
    "python jupyter-to-hpc_tf-mnist-example.py\n",
    "```\n",
    "\n",
    "Ensure both of these files are on the HPC, alongside the corresponding conda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ea8d1-52a9-4085-9008-10d1dd60ed15",
   "metadata": {
    "tags": []
   },
   "source": [
    "(create_unit_tests)=\n",
    "### (Optional) Create unit tests\n",
    "\n",
    "It's good practise to write [tests](https://the-turing-way.netlify.app/reproducible-research/testing.html) for your code.\n",
    "\n",
    "[pytest](https://docs.pytest.org/en/6.2.x/) are [NumPy](https://numpy.org/doc/stable/reference/testing.html) are both good, commonly-used tools for testing in Python.\n",
    "\n",
    "There are various [levels, types, and methods](https://softwaretestingfundamentals.com/) of testing.\n",
    "\n",
    "For example, you could [unit test](https://softwaretestingfundamentals.com/unit-testing/) individual functions, like the downloading and splitting of data:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def test_split_data(ds_train, ds_val, ds_test):\n",
    "    num_samples_train = len(ds_train)\n",
    "    num_samples_val = len(ds_val)\n",
    "    num_samples_test = len(ds_test)\n",
    "    \n",
    "    num_samples = num_samples_train + num_samples_val + num_samples_test\n",
    "    \n",
    "    np.testing.assert_almost_equal(num_samples_train / num_samples, 0.8, decimal=3)\n",
    "    np.testing.assert_almost_equal(num_samples_val / num_samples, 0.1, decimal=3)\n",
    "    np.testing.assert_almost_equal(num_samples_test / num_samples, 0.1, decimal=3)\n",
    "    \n",
    "\n",
    "test_split_data(ds_train, ds_val, ds_test)\n",
    "```\n",
    "\n",
    "You could also perform [system testing](https://softwaretestingfundamentals.com/system-testing/). For example, checking the final validation accuracy is above a threshold:\n",
    "\n",
    "```python\n",
    "def test_final_val_accuracy_above_threshold(threshold):\n",
    "    assert history.history['val_accuracy'][-1] >= threshold\n",
    "    \n",
    "test_final_val_accuracy_above_threshold(0.96)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7146e-bc2d-4078-b65e-b6b983d588e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17076a3-5a92-4917-bd8a-8c0f2f69d203",
   "metadata": {},
   "source": [
    "```{admonition} Question 1\n",
    "\n",
    "What are the two ways to parallelise machine learning, and which way is simpler?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74e521-e75b-4684-a8b3-b01951a40340",
   "metadata": {},
   "source": [
    "```{admonition} Question 2\n",
    "\n",
    "How can you check the efficiency of a CPU job?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb851c5-eb00-4bdd-a336-a70bd839b8dd",
   "metadata": {},
   "source": [
    "```{admonition} Question 3\n",
    "\n",
    "How can you check the efficiency of a GPU job?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a99796-1ff0-408c-a7b1-0b7cd64ae636",
   "metadata": {},
   "source": [
    "```{admonition} Question 4\n",
    "\n",
    "What tools can help distribute TensorFlow and PyTorch code?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1716f-1715-4b99-82e7-307ee4607ca7",
   "metadata": {},
   "source": [
    "```{admonition} Question 5\n",
    "\n",
    "In general, what should the batch size be for distributed work?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad793af-2dd4-4fe1-943e-9c0882e0039f",
   "metadata": {},
   "source": [
    "```{admonition} Question 6\n",
    "\n",
    "What are some good steps for moving Jupyter Notebook code to HPC?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8af3b-5d9a-4e2c-9bd0-223837749e07",
   "metadata": {},
   "source": [
    "## {ref}`Solutions <distributed>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85d337-9ea3-4511-ae33-4fb036418318",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de920e-c4c1-4c47-8a80-b7f1ab648754",
   "metadata": {},
   "source": [
    "```{important}\n",
    "\n",
    "- [x] _Ensure that you really need to use distributed devices._\n",
    "- [x] _Check everything first works on a single device._\n",
    "- [x] _Use data parallelism (to split the data over multiple devices)._\n",
    "- [x] _Take care when setting the global batch size._\n",
    "- [x] _Check the efficiency of your jobs to ensure utilising the requested resources._\n",
    "- [x] _When moving from Jupyter to HPC:_\n",
    "    - [x] _Clean non-essential code._\n",
    "    - [x] _Refactor Jupyter Notebook code into functions._\n",
    "    - [x] _Create a Python script._\n",
    "    - [x] _Create submission script._\n",
    "    - [x] _Create tests._\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bcbb5-e4c3-4032-b7af-8fd24d9e19e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Further information\n",
    "\n",
    "### Good practices\n",
    "\n",
    "- Ensure everything works on a single device first, _before_ going distributed.\n",
    "- Ensure that you need the overhead of distributing over multiple GPUs e.g., could you instead use 1 GPU and model checkpointing/transfer learning?\n",
    "- Ensure that the problem is complex enough to utilise multiple GPUs efficiently.\n",
    "- Batch the dataset with the global batch size e.g., for 8 devices each capable of a batch of 64 use the global batch size of 512 (= 8 * 64).  \n",
    "- [Distributed training on PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/advanced/model_parallel.html)\n",
    "- Performance tips from [PyTorch](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
    "\n",
    "### Other options\n",
    "\n",
    "- [Horovod](https://horovod.ai/)\n",
    "    - A library to make distributed deep learning fast and easy to use.\n",
    "- [DeepSpeed](https://www.deepspeed.ai/)\n",
    "    - A deep learning optimization library that makes distributed training easy, efficient, and effective.\n",
    "- [FairScale](https://fairscale.readthedocs.io/en/latest/)\n",
    "    - A PyTorch extension library for high performance and large scale training.\n",
    " \n",
    "### Resources\n",
    "\n",
    "- [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)\n",
    "- [scikit-learn parallelism](https://scikit-learn.org/stable/computing/parallelism.html)\n",
    "- [Convert ML experiment to production](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-convert-ml-experiment-to-production)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1ffad78e3b53a26aeabe29bd69865e9fcde2eed64638bf28084d4e5d53534f3"
  },
  "kernelspec": {
   "display_name": "swd8_intro_ml",
   "language": "python",
   "name": "swd8_intro_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
