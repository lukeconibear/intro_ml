{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f167c0-52a8-4800-9999-0419acc312a8",
   "metadata": {},
   "source": [
    "# Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae8044-47c9-4c48-b50d-e3dadb441757",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukeconibear/intro_ml/blob/main/docs/05_distributed.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aef6fc-d7fe-45a5-9593-f9092ae02ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're using colab, then install the required modules\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb78754a-ddf7-4ccd-a1a2-18a055f97adc",
   "metadata": {},
   "source": [
    "```{note}\n",
    "If youâ€™re in COLAB or have a local CUDA GPU, you can follow along with the more computationally intensive training in this lesson.\n",
    "\n",
    "For those in COLAB, ensure the session is using a GPU by going to: Runtime > Change runtime type > Hardware accelerator = GPU.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8aa72b-89d2-47e8-9dae-d6eff282617a",
   "metadata": {},
   "source": [
    "Distributing training over multiple devices generally uses either:\n",
    "\n",
    "- [Data parallelism](https://developers.google.com/machine-learning/glossary/#data-parallelism)\n",
    "    - Single model copied to multiple devices.\n",
    "    - Split data over multiple devices.\n",
    "    - Useful for big data.\n",
    "- [Model parallelism](https://developers.google.com/machine-learning/glossary/#model-parallelism)\n",
    "    - Split model over multiple devices.\n",
    "    - Single data copied to multiple devices.\n",
    "    - Useful for big models (for some architectures).\n",
    "    \n",
    "This lesson focuses on data parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f972d1-8eac-424a-b800-19a6c1f6ef04",
   "metadata": {},
   "source": [
    "## [Ray Train](https://docs.ray.io/en/latest/train/train.html)\n",
    "\n",
    "Ray Train simplifies distributed deep learning for TensorFlow and PyTorch.\n",
    "\n",
    "It handles the set up for you (e.g., [`TF_CONFIG`](https://www.tensorflow.org/guide/distributed_training#setting_up_the_tf_config_environment_variable) in TensorFlow).\n",
    "\n",
    "There are a range of examples [here](https://docs.ray.io/en/latest/train/examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192221c-7f41-4b0d-abc5-2d2045db01e5",
   "metadata": {},
   "source": [
    "### [TensorFlow (Keras)](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864cbb8-4483-4baa-99ea-ae49a58fa6db",
   "metadata": {},
   "source": [
    "Here is an [MNIST example](https://docs.ray.io/en/latest/train/examples/tensorflow_mnist_example.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0401135c-c7be-4ef6-83ce-4476dc9dd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "from ray.train import Trainer\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340dd1ed-38e0-455c-9f2b-08445bf302a4",
   "metadata": {},
   "source": [
    "#### [Define callback for reporting](https://docs.ray.io/en/latest/train/user_guide.html#logging-monitoring-and-callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fe996158-18c1-4d96-8d6a-f57fd9a46110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainReportCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ray.train.report(**logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e6947-55dd-4e48-a550-4213b4e16bc9",
   "metadata": {},
   "source": [
    "#### Set up the dataset and model\n",
    "\n",
    "The dataset will be split (sharded) across the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef5e61-5392-4edd-bdaf-fec50b2752cf",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "The default [auto-sharding](https://www.tensorflow.org/tutorials/distribute/input#sharding) by `FILE` can cause warning messages if the data is in one file. Instead, auto-shard by data using: `tf.data.experimental.AutoShardPolicy.DATA`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "68b1ba8c-03a4-434e-9409-8ff0e17246e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(batch_size):\n",
    "    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "    # The `x` arrays are in uint8 and have values in the [0, 255] range.\n",
    "    # You need to convert them to float32 with values in the [0, 1] range.\n",
    "    x_train = x_train / np.float32(255)\n",
    "    y_train = y_train.astype(np.int64)\n",
    "    ds_train = (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        .shuffle(60000)\n",
    "        .repeat()\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = (\n",
    "        tf.data.experimental.AutoShardPolicy.DATA\n",
    "    )\n",
    "    ds_train = ds_train.with_options(options)\n",
    "\n",
    "    return ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d5cc516c-9738-4397-97ae-309a568564e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_cnn_model(config):\n",
    "    learning_rate = config.get(\"lr\", 0.001)\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(28, 28)),\n",
    "            tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61991c2f-cf4b-4ef2-943d-bf22e3d40ba2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set up the training function for a _single_ worker\n",
    "\n",
    "You can [configure training](https://docs.ray.io/en/latest/train/user_guide.html#configuring-training) using the `config` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d2d3ca5a-badd-49b6-8cff-5b8c29c62c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    batch_size = 64\n",
    "    single_worker_dataset = mnist_dataset(batch_size)\n",
    "    single_worker_model = build_and_compile_cnn_model(config)\n",
    "    single_worker_model.fit(\n",
    "        single_worker_dataset,\n",
    "        epochs=config[\"epochs\"],\n",
    "        steps_per_epoch=70,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "405f7875-5432-4ac1-ab40-240b72d95310",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"epochs\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e6eb09cc-1b1a-4884-9d4f-026c0115816b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 2.2926 - accuracy: 0.1507\n",
      "Epoch 2/3\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 2.2321 - accuracy: 0.2761\n",
      "Epoch 3/3\n",
      "70/70 [==============================] - 1s 13ms/step - loss: 2.1645 - accuracy: 0.3920\n"
     ]
    }
   ],
   "source": [
    "train_func(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea930e9-6620-4e67-903a-52c3f76da168",
   "metadata": {},
   "source": [
    "#### [Update training function](https://docs.ray.io/en/latest/train/user_guide.html#update-training-function)\n",
    "\n",
    "1. Set the _global_ batch size\n",
    "    - Each worker will process the same size batch as in the single-worker code.\n",
    "2. Choose your TensorFlow distributed training strategy.\n",
    "    - In this example we use the [MultiWorkerMirroredStrategy](https://www.tensorflow.org/guide/distributed_training#multiworkermirroredstrategy) for synchronous training of multiple workers across many machines.\n",
    "        - For multiple workers on _one_ machine, use [MirroredStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy).\n",
    "        - In general, the mirrored strategy mirrors the parameters across the workers, ensuring replicas are identical.\n",
    "    - Within the strategy scope context manager, you build and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e8604122-4b78-4459-a49e-0716854003fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    per_worker_batch_size = config.get(\"batch_size\", 64)\n",
    "    epochs = config.get(\"epochs\", 3)\n",
    "    steps_per_epoch = config.get(\"steps_per_epoch\", 70)\n",
    "\n",
    "    tf_config = json.loads(os.environ[\"TF_CONFIG\"])\n",
    "    num_workers = len(tf_config[\"cluster\"][\"worker\"])\n",
    "\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "    global_batch_size = per_worker_batch_size * num_workers\n",
    "    multi_worker_dataset = mnist_dataset(global_batch_size)\n",
    "\n",
    "    with strategy.scope():\n",
    "        # Model building/compiling need to be within `strategy.scope()`.\n",
    "        multi_worker_model = build_and_compile_cnn_model(config)\n",
    "\n",
    "    history = multi_worker_model.fit(\n",
    "        multi_worker_dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=[TrainReportCallback()],\n",
    "        verbose=False,\n",
    "    )\n",
    "    results = history.history\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fab04c-ae80-415d-9f41-de20b03b65ae",
   "metadata": {},
   "source": [
    "#### [Create Ray Train Trainer](https://docs.ray.io/en/latest/train/user_guide.html#create-ray-train-trainer)\n",
    "\n",
    "The `Trainer` manages state and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c919384c-3a06-4cc5-a733-e98257473ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tensorflow_mnist(num_workers=1, use_gpu=False, epochs=4):\n",
    "    trainer = Trainer(backend=\"tensorflow\", num_workers=num_workers, use_gpu=use_gpu)\n",
    "    trainer.start()\n",
    "    results = trainer.run(\n",
    "        train_func=train_func, config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": epochs}\n",
    "    )\n",
    "    trainer.shutdown()\n",
    "    print(f\"Results: {results[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6ad38-736d-40d1-9025-655f838509a5",
   "metadata": {},
   "source": [
    "#### [Run the training](https://docs.ray.io/en/latest/train/user_guide.html#run-training-function)\n",
    "\n",
    "Initialise and shutdown the Ray client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5c478acf-9517-4cef-91a5-d26506e7b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "47fce682-d5af-495b-b2da-bed0feefefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu\n",
    "# train_tensorflow_mnist()\n",
    "\n",
    "# gpu\n",
    "# train_tensorflow_mnist(use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3c511b19-86e0-4a82-8e6e-48f10f51d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31ed3a-e683-4990-ab93-ed849418a360",
   "metadata": {},
   "source": [
    "This Python script is in full [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/tensorflow_ray_train_mnist_example.py).\n",
    "\n",
    "The job submission script is (also [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/distributed_ml_on_arc4_cpu.bash)):\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#$ -cwd\n",
    "#$ -l h_rt=00:30:00\n",
    "#$ -pe smp 12\n",
    "#$ -l h_vmem=6G\n",
    "\n",
    "conda activate intro_ml\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib  # (sometimes needed)\n",
    "\n",
    "python tensorflow_ray_train_mnist_example.py --num-workers 12 --epochs 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae10ba3-b47a-4974-beb7-d947eec175a0",
   "metadata": {},
   "source": [
    "In this simple example using 12 CPUs, the job efficiency (using `qacct -j <JOBID>`):\n",
    "\n",
    "```\n",
    "Efficiency = 100 * cpu / (ru_wallclock * slots)\n",
    "Efficiency = 100 * 10214 / (928 * 12)\n",
    "Efficiency = 92 %\n",
    "```\n",
    "\n",
    "92% is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7f591-53aa-4861-aea5-b14994d58d9c",
   "metadata": {},
   "source": [
    "To run on the GPU ([submission script](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/distributed_ml_on_arc4_gpu.bash)):\n",
    "- Replace `#$ -pe smp 4` with `#$ -l coproc_v100=1`.\n",
    "- Add `--use-gpu=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10195389-5b85-403a-9ae8-076c3589a536",
   "metadata": {},
   "source": [
    "### [PyTorch](https://pytorch.org/tutorials/beginner/dist_overview.html)\n",
    "\n",
    "Can also distribute with [Ray Train](https://docs.ray.io/en/latest/train/examples/train_fashion_mnist_example.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f754603-33b7-44ef-a1db-ce925579564a",
   "metadata": {},
   "source": [
    "To share data on a single filesystem, download the dataset once:\n",
    "\n",
    "```python\n",
    "Trainer(prepare_data_per_node=False)\n",
    "```\n",
    "\n",
    "The default behaviour is to download the data _once per node_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f3d77-5de1-47fb-9846-1d3becf2acc6",
   "metadata": {},
   "source": [
    "#### [DDP Strategy](https://pytorch-lightning.readthedocs.io/en/stable/advanced/model_parallel.html#ddp-optimizations)\n",
    "\n",
    "\n",
    "```python\n",
    "# train on 8 GPUs, using the DDP strategy\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp\")\n",
    "```\n",
    "...\n",
    "\n",
    "\n",
    "```python\n",
    "# train on multiple GPUs across nodes (uses 8 GPUs in total)\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=2, num_nodes=4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8fc19-bf0b-4a38-9691-196d166a303b",
   "metadata": {},
   "source": [
    "`num_workers`\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html#num-workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa4f0b-0a72-4b94-9d33-090cd551565b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a2155-16fe-4581-b866-8fb59329c722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f91ad439-f82c-4823-b862-6a8c9159f1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from typing import Dict\n",
    "\n",
    "import ray\n",
    "import torch\n",
    "from ray.train.callbacks import JsonLoggerCallback\n",
    "from ray.train.trainer import Trainer\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e5b4fb-2f70-4456-b781-1a62c0b57836",
   "metadata": {},
   "source": [
    "#### Set up the dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6cf03ff-9e75-4823-9326-85710648a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = datasets.FashionMNIST(\n",
    "#     root=\"~/data\",\n",
    "#     train=True,\n",
    "#     download=True,\n",
    "#     transform=ToTensor(),\n",
    "# )\n",
    "\n",
    "# test_data = datasets.FashionMNIST(\n",
    "#     root=\"~/data\",\n",
    "#     train=False,\n",
    "#     download=True,\n",
    "#     transform=ToTensor(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "acc028ef-e5c2-4c3c-a2df-cc89e000cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ae290f-20a3-4f3d-8f55-2e89cf4641a8",
   "metadata": {},
   "source": [
    "#### Define training and validation per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "99236486-7cd9-4c80-9afc-375522805ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset) // ray.train.world_size()\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c5c57e66-c084-4828-bc14-216f13c53e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset) // ray.train.world_size()\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n \"\n",
    "        f\"Accuracy: {(100 * correct):>0.1f}%, \"\n",
    "        f\"Avg loss: {test_loss:>8f} \\n\"\n",
    "    )\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c35dfa-5c5c-40bc-9b51-07f34399bccf",
   "metadata": {},
   "source": [
    "#### [Setup distributed training function](https://docs.ray.io/en/latest/train/user_guide.html#update-training-function)\n",
    "\n",
    "Use `ray.train.torch.prepare_model` to automatically move your model to the right device.\n",
    "\n",
    "Use `ray.train.torch.prepare_data_loader` utility functions to setup your data for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3f274752-831c-4c0b-a262-d44f83ef3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config: Dict):\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr = config[\"lr\"]\n",
    "    epochs = config[\"epochs\"]\n",
    "\n",
    "    worker_batch_size = batch_size // ray.train.world_size()\n",
    "\n",
    "    # Create data loaders.\n",
    "    train_dataloader = DataLoader(training_data, batch_size=worker_batch_size)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=worker_batch_size)\n",
    "\n",
    "    train_dataloader = ray.train.torch.prepare_data_loader(train_dataloader)\n",
    "    test_dataloader = ray.train.torch.prepare_data_loader(test_dataloader)\n",
    "\n",
    "    # Create model.\n",
    "    model = NeuralNetwork()\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    loss_results = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        train_epoch(train_dataloader, model, loss_fn, optimizer)\n",
    "        loss = validate_epoch(test_dataloader, model, loss_fn)\n",
    "        ray.train.report(loss=loss)\n",
    "        loss_results.append(loss)\n",
    "\n",
    "    return loss_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745b483-e9a7-40b3-b840-7556ce919abb",
   "metadata": {},
   "source": [
    "#### [Create Ray Train Trainer](https://docs.ray.io/en/latest/train/user_guide.html#create-ray-train-trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "48d91f56-7066-44ff-b2b6-ed6cad71dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fashion_mnist(num_workers=1, use_gpu=False):\n",
    "    trainer = Trainer(backend=\"torch\", num_workers=num_workers, use_gpu=use_gpu)\n",
    "    trainer.start()\n",
    "    result = trainer.run(\n",
    "        train_func=train_func,\n",
    "        config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": 4},\n",
    "        callbacks=[JsonLoggerCallback()],\n",
    "    )\n",
    "    trainer.shutdown()\n",
    "    print(f\"Loss results: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba0bf9-1f18-49d5-af3f-317960974cd0",
   "metadata": {},
   "source": [
    "#### [Run the training](https://docs.ray.io/en/latest/train/user_guide.html#run-training-function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8d114c39-5041-45d6-8fa5-d3c2f992ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "691b98b5-7808-41bc-90fa-26f0a4417cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu\n",
    "# train_fashion_mnist()\n",
    "\n",
    "# gpu\n",
    "# train_fashion_mnist(use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0581fc5f-492e-4eb1-bad0-1524a7b4d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93781c8-9a77-4f0a-9225-347418038ae9",
   "metadata": {},
   "source": [
    "This Python script is in full [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/pytorch_ray_train_fashion_mnist_example.py).\n",
    "\n",
    "The job submission script is the same as before ([here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/distributed_ml_on_arc4_cpu.bash)), except you use the line:\n",
    "\n",
    "```bash\n",
    "python tensorflow_ray_train_mnist_example.py --num-workers 12 --epochs 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206d6c0b-1f18-4bdb-90ca-5790ab835673",
   "metadata": {},
   "source": [
    "In this simple example using 12 CPUs, the job efficiency (using `qacct -j <JOBID>`):\n",
    "\n",
    "```\n",
    "Efficiency = 100 * cpu / (ru_wallclock * slots)\n",
    "Efficiency = 100 * X / (X * 12)\n",
    "Efficiency = X %\n",
    "```\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8d64be-5cb5-46c2-ba42-efeea5eab9c3",
   "metadata": {},
   "source": [
    "To run on the GPU ([submission script](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/distributed_ml_on_arc4_gpu.bash)):\n",
    "- Replace `#$ -pe smp 4` with `#$ -l coproc_v100=1`.\n",
    "- Add `--use-gpu=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf4c2d-1031-47df-a076-863634cf360d",
   "metadata": {},
   "source": [
    "## [PyTorch (Lightning)](https://pytorch-lightning.readthedocs.io/en/stable/clouds/cluster.html)\n",
    "\n",
    "\n",
    "[SLURM](https://pytorch-lightning.readthedocs.io/en/stable/clouds/cluster.html#slurm-managed-cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950b82ad-dd2c-46f2-a24d-4d2b01ccf371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "def main(hparams):\n",
    "    model = LightningTemplateModel(hparams)\n",
    "\n",
    "    trainer = Trainer(accelerator=\"gpu\", devices=8, num_nodes=4, strategy=\"ddp\")\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    parent_parser = ArgumentParser(add_help=False)\n",
    "    hyperparams = parser.parse_args()\n",
    "\n",
    "    # TRAIN\n",
    "    main(hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91254bc-9913-4fb7-8a30-bcf77e85d638",
   "metadata": {},
   "source": [
    "```bash\n",
    "# (submit.sh)\n",
    "#!/bin/bash -l\n",
    "\n",
    "# SLURM SUBMIT SCRIPT\n",
    "#SBATCH --nodes=4\n",
    "#SBATCH --gres=gpu:8\n",
    "#SBATCH --ntasks-per-node=8\n",
    "#SBATCH --mem=0\n",
    "#SBATCH --time=0-02:00:00\n",
    "\n",
    "# activate conda env\n",
    "source activate $1\n",
    "\n",
    "# debugging flags (optional)\n",
    "export NCCL_DEBUG=INFO\n",
    "export PYTHONFAULTHANDLER=1\n",
    "\n",
    "# on your cluster you might need these:\n",
    "# set the network interface\n",
    "# export NCCL_SOCKET_IFNAME=^docker0,lo\n",
    "\n",
    "# might need the latest CUDA\n",
    "# module load NCCL/2.4.7-1-cuda.10.0\n",
    "\n",
    "# run script from above\n",
    "srun python3 train.py\n",
    "```\n",
    "\n",
    "```\n",
    "sbatch submit.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbf7c5-3637-437b-86db-afd4b6e041e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b60d3-1e43-4356-bc87-caae8c118158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d9ea8d1-52a9-4085-9008-10d1dd60ed15",
   "metadata": {},
   "source": [
    "## Jupyter Notebook to HPC\n",
    "\n",
    "It's preferable to use a static job on the HPC. To do this, you could test out different ideas locally in a Jupyter Notebook, then when ready convert this to an executable script (`.py`) and move it over. \n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ff16b1-ed60-47ee-a16d-19acfdda17dd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42a7146e-bc2d-4078-b65e-b6b983d588e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74e521-e75b-4684-a8b3-b01951a40340",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 1\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8af3b-5d9a-4e2c-9bd0-223837749e07",
   "metadata": {},
   "source": [
    "## {ref}`Solutions <distributed>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85d337-9ea3-4511-ae33-4fb036418318",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de920e-c4c1-4c47-8a80-b7f1ab648754",
   "metadata": {},
   "source": [
    "```{important}\n",
    "\n",
    "- [x] _..._\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bcbb5-e4c3-4032-b7af-8fd24d9e19e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Further information\n",
    "\n",
    "### Good practices\n",
    "\n",
    "- Ensure works on a single workers first, _before_ going distributed.\n",
    "- Ensure that you need the overhead of distributing over multiple GPUs e.g., could you instead use 1 GPU and model checkpointing?\n",
    "- Ensure that the problem is complex enough to use multiple GPUs efficiently.\n",
    "- Batch the dataset with the global batch size e.g., for 8 devices each capable of a btach of 64 use the global batch size of 512 (= 8 * 64).  \n",
    "- ...\n",
    "\n",
    "### Other options\n",
    "\n",
    "- [Horovod](https://horovod.ai/)\n",
    "- [DeepSpeed](https://www.deepspeed.ai/)\n",
    " \n",
    "### Resources\n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c2701-5bd2-4455-802f-6ae6ff8a70fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1ffad78e3b53a26aeabe29bd69865e9fcde2eed64638bf28084d4e5d53534f3"
  },
  "kernelspec": {
   "display_name": "swd8_intro_ml",
   "language": "python",
   "name": "swd8_intro_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
