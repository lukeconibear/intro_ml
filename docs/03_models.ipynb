{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f167c0-52a8-4800-9999-0419acc312a8",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae8044-47c9-4c48-b50d-e3dadb441757",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukeconibear/intro_ml/blob/main/docs/03_models.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49aef6fc-d7fe-45a5-9593-f9092ae02ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're using colab, then install the required modules\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f957d13-2a72-42fc-8021-bc74c284678a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f0e60c1-18e5-485c-ab4a-53a91606ad41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f196e67-e94a-45c6-a5b5-0660e5fb27ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c24c725f-0aed-4b26-b2c0-6e02de0ad324",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59117e95-cf70-4c7e-b667-fe146a31511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 16:25:28.299470: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-14 16:25:28.299487: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea665a2d-75b9-44d7-9cc9-62eb46b82ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, GPUs are not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 16:25:29.209000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-14 16:25:29.209027: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-14 16:25:29.209044: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (UOL-LAP-5G6CZH3): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"Yes, there are {len(tf.config.list_physical_devices('GPU'))} GPUs available.\")\n",
    "else:\n",
    "    print('No, GPUs are not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1f785-8b2f-4c64-9b55-af983de58c9e",
   "metadata": {},
   "source": [
    "[Keras Applications](https://keras.io/api/applications/)\n",
    "\n",
    "- DenseNet\n",
    "- VGG\n",
    "- EfficientNet\n",
    "- MobileNet\n",
    "- Inception\n",
    "- ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cc9ec15-8e47-4038-8105-5df5229a97be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclassifier_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Instantiates the Inception v3 architecture.\n",
       "\n",
       "Reference:\n",
       "- [Rethinking the Inception Architecture for Computer Vision](\n",
       "    http://arxiv.org/abs/1512.00567) (CVPR 2016)\n",
       "\n",
       "This function returns a Keras image classification model,\n",
       "optionally loaded with weights pre-trained on ImageNet.\n",
       "\n",
       "For image classification use cases, see\n",
       "[this page for detailed examples](\n",
       "  https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n",
       "\n",
       "For transfer learning use cases, make sure to read the\n",
       "[guide to transfer learning & fine-tuning](\n",
       "  https://keras.io/guides/transfer_learning/).\n",
       "\n",
       "Note: each Keras Application expects a specific kind of input preprocessing.\n",
       "For `InceptionV3`, call `tf.keras.applications.inception_v3.preprocess_input`\n",
       "on your inputs before passing them to the model.\n",
       "`inception_v3.preprocess_input` will scale input pixels between -1 and 1.\n",
       "\n",
       "Args:\n",
       "  include_top: Boolean, whether to include the fully-connected\n",
       "    layer at the top, as the last layer of the network. Default to `True`.\n",
       "  weights: One of `None` (random initialization),\n",
       "    `imagenet` (pre-training on ImageNet),\n",
       "    or the path to the weights file to be loaded. Default to `imagenet`.\n",
       "  input_tensor: Optional Keras tensor (i.e. output of `layers.Input()`)\n",
       "    to use as image input for the model. `input_tensor` is useful for sharing\n",
       "    inputs between multiple different networks. Default to None.\n",
       "  input_shape: Optional shape tuple, only to be specified\n",
       "    if `include_top` is False (otherwise the input shape\n",
       "    has to be `(299, 299, 3)` (with `channels_last` data format)\n",
       "    or `(3, 299, 299)` (with `channels_first` data format).\n",
       "    It should have exactly 3 inputs channels,\n",
       "    and width and height should be no smaller than 75.\n",
       "    E.g. `(150, 150, 3)` would be one valid value.\n",
       "    `input_shape` will be ignored if the `input_tensor` is provided.\n",
       "  pooling: Optional pooling mode for feature extraction\n",
       "    when `include_top` is `False`.\n",
       "    - `None` (default) means that the output of the model will be\n",
       "        the 4D tensor output of the last convolutional block.\n",
       "    - `avg` means that global average pooling\n",
       "        will be applied to the output of the\n",
       "        last convolutional block, and thus\n",
       "        the output of the model will be a 2D tensor.\n",
       "    - `max` means that global max pooling will be applied.\n",
       "  classes: optional number of classes to classify images\n",
       "    into, only to be specified if `include_top` is True, and\n",
       "    if no `weights` argument is specified. Default to 1000.\n",
       "  classifier_activation: A `str` or callable. The activation function to use\n",
       "    on the \"top\" layer. Ignored unless `include_top=True`. Set\n",
       "    `classifier_activation=None` to return the logits of the \"top\" layer.\n",
       "    When loading pretrained weights, `classifier_activation` can only\n",
       "    be `None` or `\"softmax\"`.\n",
       "\n",
       "Returns:\n",
       "  A `keras.Model` instance.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/intro_ml/lib/python3.9/site-packages/keras/applications/inception_v3.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.keras.applications.inception_v3.InceptionV3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e195f4c-7d97-4ec8-aac4-285d8bbe0949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a046d-a96b-40ba-8081-f8113ee54a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3865610-ea8c-45a7-afd8-e19af2d427e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 16:25:29.237337: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58892288/58889256 [==============================] - 5s 0us/step\n",
      "58900480/58889256 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "conv_base = tf.keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(180, 180, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6fd2952-8c1f-424e-9f97-1a12204ae9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 180, 180, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 180, 180, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 90, 90, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 90, 90, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 90, 90, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 45, 45, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 45, 45, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 22, 22, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 22, 22, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 11, 11, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeea6a8-9fb4-489d-b4b9-9de83bf322c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc151796-56f6-49bc-891b-fb4d402477a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca10882-a972-410b-b556-f14e2363cb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7d4346-f729-49c1-b1dd-734b1a1d1430",
   "metadata": {},
   "source": [
    "## Pretrained\n",
    "\n",
    "...\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ab731-0a78-4849-b1f6-a9cfcfdd1de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0142d-2014-4bd8-96dc-29271468e750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42a7146e-bc2d-4078-b65e-b6b983d588e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74e521-e75b-4684-a8b3-b01951a40340",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 1\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8af3b-5d9a-4e2c-9bd0-223837749e07",
   "metadata": {},
   "source": [
    "## {ref}`Solutions <models>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85d337-9ea3-4511-ae33-4fb036418318",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de920e-c4c1-4c47-8a80-b7f1ab648754",
   "metadata": {},
   "source": [
    "```{important}\n",
    "\n",
    "- [x] _..._\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bcbb5-e4c3-4032-b7af-8fd24d9e19e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Further information\n",
    "\n",
    "### Good practices\n",
    "\n",
    "- See if there is a model architecture (and parameters) that already addresses the task.\n",
    "- ...\n",
    "\n",
    "### Other options\n",
    "\n",
    "- ...\n",
    " \n",
    "### Resources\n",
    "\n",
    "- [TensorFlow Hub](https://tfhub.dev/)\n",
    "    - A comprehensive repository of trained models ready for fine-tuning and deployable anywhere.\n",
    "- [TensorFlow Model Garden](https://github.com/tensorflow/models/tree/master/official)\n",
    "    - Machine learning models and examples built with TensorFlow's high-level APIs.\n",
    "- [Torch Vision Models](https://pytorch.org/vision/stable/models.html)\n",
    "- [PyTorch Image Models](https://rwightman.github.io/pytorch-image-models/)\n",
    "- [Model Zoo](https://modelzoo.co/)\n",
    "    - Discover open source deep learning code and pretrained models.\n",
    "- [Papers with code - Models](https://paperswithcode.com/methods)\n",
    "- [HuggingFace - Models](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae56ea9-8437-4cae-9de9-f0b74538d924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1ffad78e3b53a26aeabe29bd69865e9fcde2eed64638bf28084d4e5d53534f3"
  },
  "kernelspec": {
   "display_name": "intro_ml",
   "language": "python",
   "name": "intro_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
