{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f167c0-52a8-4800-9999-0419acc312a8",
   "metadata": {},
   "source": [
    "# Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae8044-47c9-4c48-b50d-e3dadb441757",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukeconibear/intro_ml/blob/main/docs/01_fundamentals.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49aef6fc-d7fe-45a5-9593-f9092ae02ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're using colab, then install the required modules\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2ed88-bc0e-4ee8-a66e-ec2b09101c6c",
   "metadata": {},
   "source": [
    "## Basic ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fac813-143f-438d-a409-71865b75abb6",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Machine learning is a subset of Artificial Intelligence.\n",
    "\n",
    "It is a range of methods that learn associations from data.\n",
    "\n",
    "It then uses these associations for new predictions.\n",
    "\n",
    "These can be useful for:\n",
    "\n",
    "- Prediction problems (e.g., pattern recognition).\n",
    "- Problems cannot program (e.g., image recognition).\n",
    "- Faster approximations to problems that can program (e.g., spam classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90656b-a1a9-4b86-bb73-f186f5f170c6",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "Within machine learning, there are many different methods.\n",
    "\n",
    "Some main methods are:\n",
    "\n",
    "- Classic\n",
    "- Deep learning (neural networks)\n",
    "- Reinforcement learning\n",
    "- Ensembles (e.g., multiple decision trees)\n",
    "\n",
    "We'll focus on _classic machine learning_ and _deep learning_ in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877e12d-cefa-4af4-adfb-6aae07e3db3b",
   "metadata": {},
   "source": [
    "### Classic Machine Learning\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "Simple data, clear features.\n",
    "\n",
    "There are a wide variety of types. Some common ones are:\n",
    "\n",
    "- Linear models\n",
    "    - ...\n",
    "- Nearest neighbours\n",
    "    - Points are similar to their neighbours.\n",
    "- Decision trees\n",
    "    - Split the data by a decision (i.e., a branch of leaves).\n",
    "    - Combine multiple decisions (i.e., a tree). \n",
    "- Support vector machines\n",
    "    - ...\n",
    "- And many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895785bf-2f2e-4ac2-bf73-22e562d566bf",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "\n",
    "Deep means more layers.\n",
    "\n",
    "\n",
    "\n",
    "Neural networks \n",
    "\n",
    "Useful for non-linear, with large number of features \n",
    "\n",
    "Compilcated data, unclear features.\n",
    "\n",
    "\n",
    "\n",
    "There are a wide variety of types. Some common ones are:\n",
    "\n",
    "- Convolutional Neural Networks (CNN)\n",
    "    - ...\n",
    "- Recurrent Neural Networks (RNN)\n",
    "    - For sequential data e.g., time-series, natural language.\n",
    "    - Loops over timesteps while maintaining information from previous timesteps.\n",
    "- Transformers\n",
    "    - ...\n",
    "- Sequence\n",
    "    - ...\n",
    "- Generative Adverserial Networks (GANs)\n",
    "    - ...\n",
    "- And many more.\n",
    "\n",
    "...\n",
    "\n",
    "Steps \n",
    "\n",
    "Inputs \n",
    "\n",
    "forward propagate \n",
    "\n",
    "predict outputs \n",
    "\n",
    "compute loss \n",
    "\n",
    "backward propagate \n",
    "\n",
    "gradient descent \n",
    "\n",
    "update weights and biases \n",
    "\n",
    "\n",
    "Scale is driving DL progress \n",
    "\n",
    "Bigger training data (Larger data sets (labelled, m)) \n",
    "\n",
    "Bigger neural networks \n",
    "\n",
    "Now investment and attention drive it forward more \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c27bd69-5f3b-4119-9a92-91ec80ffa5c0",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The data is a sample of the problem you're studying.\n",
    "\n",
    "Data has inputs (features) and outputs (targets).\n",
    "\n",
    "- The inputs are what you provide to the model.\n",
    "- The outputs are what you're trying to predict.\n",
    "\n",
    "The data is normally in the form of tensors.\n",
    "\n",
    "Tensors are multi-dimensional arrays:\n",
    "\n",
    "- Scalars are rank-0 tensors.\n",
    "- Vectors are rank-1 tensors.\n",
    "- Matrices are rank-2 tensors.\n",
    "- 3+ dimensional arrays are rank-3+ tensors.\n",
    "\n",
    "![tensors.png](images/tensors.png)  \n",
    "\n",
    "*[Image source](https://medium.com/mlait/tensors-representation-of-data-in-neural-networks-bbe8a711b93b)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab95c87-9367-4ceb-a4ba-4b9864f8888d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Supervised and unsupervised\n",
    "\n",
    "- Supervised learning is when you provide labelled outputs to learn from.\n",
    "- Unsupervised learning when you don't provide any labels.\n",
    "\n",
    "Below is an example of supervised learning (classify different coloured markers) and unsupervised learning (find clusters within similar data).\n",
    "\n",
    "![supervised_vs_unsupervised.png](images/supervised_vs_unsupervised.png)  \n",
    "\n",
    "*[Image source](https://analystprep.com/study-notes/cfa-level-2/quantitative-method/supervised-machine-learning-unsupervised-machine-learning-deep-learning/)*\n",
    "\n",
    "We'll focus on supervised learning in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11f0eb-94cb-480e-b8bc-210e080de6fe",
   "metadata": {},
   "source": [
    "### Classification and regression\n",
    "\n",
    "- Classification problems are those that try to predict a discrete category (i.e., cat or dog).\n",
    "- Regression problems are those that try to predict a continuous number (i.e., beans in a jar).\n",
    "\n",
    "Below is an example of classification (separate blue circles from purple crosses) and regression (predict a numerical value from the data).\n",
    "\n",
    "![classification_vs_regression.png](images/classification_vs_regression.png)  \n",
    "\n",
    "*[Image source](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c85c5-47ff-4026-8821-7cbb6655771e",
   "metadata": {},
   "source": [
    "### Training, validation, and test splits\n",
    "\n",
    "The data is normally split into training, validation, and test sets.\n",
    "\n",
    "- The training set is for training the model.\n",
    "- The validation set (optional) is for iteratively optimising the model during training.\n",
    "- The test set is only for testing the model at the end.\n",
    "    - This should remain untouched and _single-use_ (to ensure representative of future data).\n",
    "\n",
    "![train-val-test-split.png](images/train-val-test-split.png)  \n",
    "\n",
    "*[Image source](https://stackoverflow.com/a/56100053/6250873)*\n",
    "\n",
    "The size of the split depends on the size of the dataset and the signal you're trying to predict (i.e., the smaller the signal, then the larger the test set needs to be).\n",
    "\n",
    "- For small data sets, a split of 60/20/20 for train/validation/test may be suitable.\n",
    "- For medium data sets, a split of 80/10/10 for train/validation/test may be suitable.\n",
    "- For large data sets, a split of 90/5/5 for train/validation/test may be suitable.\n",
    "- For very large data sets, a split of 98/1/1 for train/validation/test may be suitable.\n",
    "\n",
    "The split may benefit from being stratified to ensure each set has a sample of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575d94e-ed36-4330-94eb-9a398a9c04a4",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "To estimate the _variability_ in the training score, then you can use cross-validation.\n",
    "\n",
    "This repeats the _training/validation_ split multiple times (_the test data remains untouched_).\n",
    "\n",
    "There are various methods for cross-validation.\n",
    "\n",
    "These are mainly variations of K-fold cross-validation, where you split the data up k times (e.g., 5).\n",
    "\n",
    "Variations then consider stratifying (preserving original class frequencies), shuffling, sampling, and replacing.\n",
    "\n",
    "Below is an example for 5-fold cross-validation (i.e., splitting 5 times).\n",
    "\n",
    "![cross_validation.png](images/cross_validation_diagram.png)  \n",
    "\n",
    "*[Image source](https://inria.github.io/scikit-learn-mooc/python_scripts/02_numerical_pipeline_cross_validation.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdc45f-a498-43cd-95fc-c2bc576c4594",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "These are what _you set before_ model training (i.e., the architecture).\n",
    "\n",
    "They control the learning process.\n",
    "\n",
    "They are often found through iterative trying out different options.\n",
    "\n",
    "This iterative tuning method can be:\n",
    "\n",
    "- Systematically over a grid (i.e., grid-search)\n",
    "    - Thorough, slow, not suitable for problems with many variables\n",
    "- Randomly over a grid (i.e., random grid-search)\n",
    "    - Faster and more suitable for problems with many variables\n",
    "- Other options including:\n",
    "    - Using Bayes Theorem (i.e., Bayes grid-search) to choose a new set of hyperparameters to test based on the performance of prior set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa8ea7-c9ed-45b2-8415-86cbe1a0d5af",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "These are what the model learns _during training_ (i.e., the weights / biases / coefficients of the model).\n",
    "\n",
    "\n",
    "Best set of parameters (i.e., global optimimum)\n",
    "local optimum\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce269f9-07be-4288-bdcd-dd9fd6e39c5b",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "- Optimiser\n",
    "- Loss function (error on single training example) \n",
    "    - always want to minimise\n",
    "    - a proxy of the metric with a smooth gradient (in some cases is actually the same as the metric e.g., mean squared error)\n",
    "- Metric\n",
    "\n",
    "...\n",
    "\n",
    "- Cost function (average of loss functions over whole training set) \n",
    "- gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813f611-078f-4c96-a923-49ff5bba8f49",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "The goal of machine learning is predicting new data.\n",
    "\n",
    "Hence, the objective is to minimise the _test error_ (as this represents new data).\n",
    "\n",
    "...\n",
    "\n",
    "- error analysis (e.g., Confusion Matrix)\n",
    "\n",
    "...\n",
    "\n",
    "R2 (coefficient of determination) \n",
    "Any value less than 1, as model can be continually awful \n",
    "1 is perfect \n",
    "0 is not more information than just predicting the mean \n",
    "\n",
    "Evaluation metric is the goal of training.\n",
    "\n",
    "- One singular evaluation metric to help guide decisions\n",
    "\n",
    "Classification\n",
    "\n",
    "- Class imbalance\n",
    "\n",
    "Regression\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6906e8-bf84-440d-9a2b-c6f77bc7dc87",
   "metadata": {},
   "source": [
    "### Underfit\n",
    "\n",
    "A model _underfits_ the data when it has _high bias_ (i.e., systematic errors). \n",
    "\n",
    "This means the model is _too simple_ to capture the association.\n",
    "\n",
    "You can tell that the model underfits because there are _both_ high training errors and high test errors.\n",
    "\n",
    "To reduce underfitting, try:\n",
    "\n",
    "- Adding more features.\n",
    "- Adding more complex features.\n",
    "- Decreasing regularisation (i.e., decrease preference for simpler functions).\n",
    "\n",
    "More training data is unlikely to help a model that underfits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b5ce0-3c5a-4107-8dd0-1376e1e370d9",
   "metadata": {},
   "source": [
    "### Overfit\n",
    "\n",
    "A model _overfits_ the data when it has _high variance_ (i.e., varies a lot). \n",
    "\n",
    "This means the model is _too complex_ to capture the association.\n",
    "\n",
    "You can tell that the model overfits because there are _low_ training errors _but_ high test errors (i.e., there is a big difference between these errors, where the model doesn't work well on new data because it overfitted to the noise in the training data).\n",
    "\n",
    "To reduce overfitting, try:\n",
    "\n",
    "- Adding more data.\n",
    "- Using fewer or simpler features.\n",
    "- Increasing regularisation (i.e., increase preference for simpler functions).\n",
    "- A smaller neural network with fewer layers/parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca47611-8328-44bc-94e9-15681ac361df",
   "metadata": {},
   "source": [
    "Below is an example of underfitting (linear line through non-linear data) and overfitting (very-high order polynomial passing through every training point).\n",
    "\n",
    "![underfit_vs_overfit.png](images/underfit_vs_overfit.png)  \n",
    "\n",
    "*[Image source](https://www.educative.io/edpresso/overfitting-and-underfitting)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7146e-bc2d-4078-b65e-b6b983d588e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74e521-e75b-4684-a8b3-b01951a40340",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 1\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8af3b-5d9a-4e2c-9bd0-223837749e07",
   "metadata": {},
   "source": [
    "## {ref}`Solutions <fundamentals>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85d337-9ea3-4511-ae33-4fb036418318",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de920e-c4c1-4c47-8a80-b7f1ab648754",
   "metadata": {},
   "source": [
    "```{important}\n",
    "\n",
    "- [x] _..._\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970bc04-88fc-4cf2-89b3-7b4cdc8bc1f8",
   "metadata": {},
   "source": [
    "## Further information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b8126-a770-470e-af61-023eea1593fa",
   "metadata": {},
   "source": [
    "### Good practices\n",
    "\n",
    "- The choice of algortihm depends on the problem/data (i.e., whether you use linear regression, deep learning, etc.).\n",
    "    - What assumptions are appropriate?\n",
    "- Future data should be from the same distribution as the training data (_data drift_).\n",
    "- The test set should be representative of the future data. For example:\n",
    "    - For time series, test data may be 2021, while training data was 2015-2020. \n",
    "    - For medical application, test data may be completely new patients, not multiple visits from same patients in training data.\n",
    "- Consider reducing the dimensionality of the data (e.g., using PCA, Principle Component Analysis).\n",
    "- Have a baseline to compare the model skill against (i.e., simple model, human performance, etc.).\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446b2ff-d818-4880-a89e-a6eac6a900f6",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "\n",
    "- Predictions are primarily based on associations, not explanations or causation.\n",
    "- Predictions and models are specific to the data they were trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c5a0f-2f82-441c-9097-32417fff0152",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "**Bold** are highly-recommended.\n",
    "\n",
    "- **[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), Aurélien Géron, 2019, O’Reilly Media, Inc.**  \n",
    "    - **[Jupyter notebooks](https://github.com/ageron/handson-ml2).**  \n",
    "- [Deep Learning with Python, 2nd Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff), François Chollet, 2021, Manning.  \n",
    "    - [Jupyter notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks).  \n",
    "- [Artificial Intelligence: A Modern Approach, 4th edition](http://aima.cs.berkeley.edu/), Stuart Russell and Peter Norvig, 2021, Pearson.  \n",
    "- [Machine Learning Yearning](https://www.deeplearning.ai/programs/), Andrew Ng.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be3f20-4fb0-47c3-9b96-0a2f29dfd261",
   "metadata": {},
   "source": [
    "(online_courses)=\n",
    "### Online courses\n",
    "\n",
    "**Bold** are highly-recommended.\n",
    "\n",
    "#### Machine learning\n",
    "\n",
    "- **[Machine learning](https://www.coursera.org/learn/machine-learning), Coursera, Andrew Ng.**\n",
    "    - **CS229, Stanford University: [Video lectures](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU).**  \n",
    "- **[Machine Learning for Intelligent Systems](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/), Kilian Weinberger, 2018.**  \n",
    "    - **CS4780, Cornell: [Video lectures](https://youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS).**  \n",
    "- [Artificial Intelligence: Principles and Techniques](https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX), Percy Liang and Dorsa Sadigh, CS221, Standord, 2019.  \n",
    "- [Machine learning in Python with scikit-learn](https://www.fun-mooc.fr/en/courses/machine-learning-python-scikit-learn/), scikit-learn developers, 2022.\n",
    "  - [Course materials](https://inria.github.io/scikit-learn-mooc/)\n",
    "  - [Jupyter Notebooks](https://github.com/INRIA/scikit-learn-mooc/) \n",
    "\n",
    "\n",
    "#### Deep learning\n",
    "\n",
    "- **[Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning), Coursera, DeepLearning.AI (_NumPy, Keras, TensorFlow_)**\n",
    "    - **CS230, Stanford University: [Video lectures](https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb), [Syllabus](http://cs230.stanford.edu/syllabus/)**\n",
    "- [NYU Deep Learning](https://atcold.github.io/NYU-DLSP21/), Yann LeCun and Alfredo Canziani, NYU, 2021 (_PyTorch_)\n",
    "    - [Video lectures](https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1ffad78e3b53a26aeabe29bd69865e9fcde2eed64638bf28084d4e5d53534f3"
  },
  "kernelspec": {
   "display_name": "intro_ml",
   "language": "python",
   "name": "intro_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
