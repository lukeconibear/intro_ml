{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f167c0-52a8-4800-9999-0419acc312a8",
   "metadata": {},
   "source": [
    "# Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2ed88-bc0e-4ee8-a66e-ec2b09101c6c",
   "metadata": {},
   "source": [
    "## Basic ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fac813-143f-438d-a409-71865b75abb6",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Machine learning is a subset of Artificial Intelligence.\n",
    "\n",
    "It is a range of methods that learn associations from (training) data.\n",
    "\n",
    "It then uses these associations for new predictions (i.e., also known as [inference](https://developers.google.com/machine-learning/glossary/#inference)).\n",
    "\n",
    "This ability to do this well is [generalising](https://developers.google.com/machine-learning/glossary/#generalization).\n",
    "\n",
    "These can be useful for a range of problems including:\n",
    "\n",
    "- Prediction problems (e.g., pattern recognition).\n",
    "- Problems that you cannot (or are difficult to) program (e.g., image recognition).\n",
    "- Faster approximations to problems that you can program (e.g., spam classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061e7d0-1e87-4013-a178-3e5d2ef2d7b6",
   "metadata": {},
   "source": [
    "```{image} images/ai_ml_dl.jpg\n",
    ":height: 300px\n",
    ":name: ai_ml_dl.jpg\n",
    "```\n",
    "\n",
    "*[Image source](https://vas3k.com/blog/machine_learning/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90656b-a1a9-4b86-bb73-f186f5f170c6",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "Within machine learning, there are many different methods.\n",
    "\n",
    "We'll focus on _classic machine learning_ and _deep learning_ in this course.\n",
    "\n",
    "```{image} images/ml_types.jpg\n",
    ":height: 300px\n",
    ":name: ml_types.jpg\n",
    "```\n",
    "\n",
    "*[Image source](https://vas3k.com/blog/machine_learning/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877e12d-cefa-4af4-adfb-6aae07e3db3b",
   "metadata": {},
   "source": [
    "### Classic Machine Learning\n",
    "\n",
    "There are a wide variety of types. Some common ones are:\n",
    "\n",
    "- [Linear Regression](https://youtu.be/kHwlB_j7Hkc)\n",
    "    - Predict a continuous number using a linear model i.e., fit a straight line.\n",
    "- [Logistic Regression](https://youtu.be/hjrYrynGWGA)\n",
    "    - Predict a class of either 0 or 1 i.e., a binary classification problem.\n",
    "- [Clustering](https://youtu.be/hDmNF9JG3lo)\n",
    "    - Predictions are based on their similarility to their neighbours.\n",
    "- [Support vector machines](https://youtu.be/hCOIMkcsm_g)\n",
    "    - Predictions are based their position relative to a decision boundary.\n",
    "    - The decision boundary is found by focusing on the two hardest to classify examples and placing support vectors between them.\n",
    "- And many, many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895785bf-2f2e-4ac2-bf73-22e562d566bf",
   "metadata": {},
   "source": [
    "### Deep Learning (Neural Networks)\n",
    "\n",
    "[Neural networks](https://youtu.be/n1l-9lIMW7E) are models made of layers of neurons.\n",
    "\n",
    "[Neurons](https://developers.google.com/machine-learning/glossary/#neuron) (also known as units or nodes) take in inputs and return an output by applying an activation function.\n",
    "\n",
    "[Activation functions](https://youtu.be/Xvg00QnyaIY) (also known as non-linearities) take a weighted sum of inputs from a previous layer, apply a non-linear function, and pass the output onto the next layer.\n",
    "\n",
    "Common activation functions are:\n",
    "\n",
    "- [ReLU (Rectified Linear Unit)](https://developers.google.com/machine-learning/glossary/#rectified-linear-unit-relu)\n",
    "    - If input is negative, the output equals 0.\n",
    "    - If input is positive, the output equals the input.\n",
    "- [Sigmoid](https://developers.google.com/machine-learning/glossary/#sigmoid-function)\n",
    "    - Converts [log-odds](logits_and_log_odds) (we'll see these later) into probabilities between 0 and 1.\n",
    "    - Used for binary classification.\n",
    "- [Softmax](https://developers.google.com/machine-learning/glossary/#softmax)\n",
    "    - Sigmoid for multi-classification.\n",
    "\n",
    "The neurons are connected in [many layers](https://developers.google.com/machine-learning/glossary/#hidden-layer).\n",
    "\n",
    "Each layer is an input-output transformation.\n",
    "\n",
    "All the layers together are the model.\n",
    "\n",
    "The hidden layers between the [input layer](https://developers.google.com/machine-learning/glossary/#input-layer) and [output layer](https://developers.google.com/machine-learning/glossary/#output-layer) are the depth of the model (hence, _deep_ learning). \n",
    "\n",
    "A common layer is for all the neurons to be [fully connected](https://developers.google.com/machine-learning/glossary/#fully-connected-layer) to each other (also known as a dense layer).\n",
    "\n",
    "The types of layers, how many there are, and how they are connected is the architecture of the neural network.\n",
    "\n",
    "There are a wide variety of types of neural networks. Some common ones are:\n",
    "\n",
    "- [Convolutional Neural Networks (CNN)](https://youtu.be/3PyJA9AfwSk)\n",
    "    - A neural network that uses convolutional layers.\n",
    "    - [Convolutional layers](https://youtu.be/jPOAS7uCODQ) group operations to reduce the number of parameters learned.\n",
    "    - They're useful for image recognition problems.\n",
    "- [Recurrent Neural Networks (RNN)](https://developers.google.com/machine-learning/glossary/#recurrent-neural-network)\n",
    "    - For sequential data e.g., time-series, natural language.\n",
    "    - Loops over timesteps while maintaining information from previous timesteps.\n",
    "- And many, many more.\n",
    "\n",
    "[Deep learning has been progressing](https://youtu.be/xflCLdJh0n0) primarily due to scale (bigger datasets and bigger neural networks), investment, and attention (additional research)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c27bd69-5f3b-4119-9a92-91ec80ffa5c0",
   "metadata": {},
   "source": [
    "(tensors)=\n",
    "### [Data](https://developers.google.com/machine-learning/glossary/#data-set-or-dataset)\n",
    "\n",
    "The data is a sample of the problem you're studying.\n",
    "\n",
    "Data has inputs (also known as features) and outputs (also known as targets).\n",
    "\n",
    "- The inputs are what you provide to the model.\n",
    "- The outputs are what you're trying to predict.\n",
    "\n",
    "The data is normally in the form of tensors.\n",
    "\n",
    "[Tensors](https://developers.google.com/machine-learning/glossary/#tensor) are multi-dimensional arrays:\n",
    "\n",
    "- Scalars are rank-0 tensors.\n",
    "- Vectors are rank-1 tensors.\n",
    "- Matrices are rank-2 tensors.\n",
    "- 3+ dimensional arrays are rank-3+ tensors.\n",
    "\n",
    "![tensors.png](images/tensors.png)  \n",
    "\n",
    "*[Image source](https://medium.com/mlait/tensors-representation-of-data-in-neural-networks-bbe8a711b93b)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76ecd6-3fed-4d35-b052-37633c8d7153",
   "metadata": {},
   "source": [
    "(logits_and_log_odds)=\n",
    "#### Logits and Log-odds\n",
    "\n",
    "[Logits](https://developers.google.com/machine-learning/glossary#logits) are a vector of raw (non-normalised) predictions from a classification model. For multi-class classification, these are converted to (normalised) probabilities using a softmax function.\n",
    "\n",
    "[Log-odds](https://developers.google.com/machine-learning/glossary#log-odds) are the logarithm of the odds of an event. They're the inverse of the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab95c87-9367-4ceb-a4ba-4b9864f8888d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Supervised and Unsupervised\n",
    "\n",
    "- [Supervised learning](https://developers.google.com/machine-learning/glossary/#supervised-machine-learning) is when you provide [labelled](https://developers.google.com/machine-learning/glossary/#label) outputs to learn from.\n",
    "- [Unsupervised learning](https://developers.google.com/machine-learning/glossary/#unsupervised-machine-learning) when you don't provide any labels.\n",
    "\n",
    "Below is an example of supervised learning (classify different coloured markers) and unsupervised learning (find clusters within data).\n",
    "\n",
    "![supervised_vs_unsupervised.png](images/supervised_vs_unsupervised.png)  \n",
    "\n",
    "*[Image source](https://analystprep.com/study-notes/cfa-level-2/quantitative-method/supervised-machine-learning-unsupervised-machine-learning-deep-learning/)*\n",
    "\n",
    "We'll focus on supervised learning in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf11f0eb-94cb-480e-b8bc-210e080de6fe",
   "metadata": {},
   "source": [
    "### Classification and Regression\n",
    "\n",
    "- [Classification](https://developers.google.com/machine-learning/glossary/#classification-model) problems are those that try to predict a [discrete category](https://developers.google.com/machine-learning/glossary/#categorical-data).\n",
    "    - i.e., binary: cat or dog, multi-class: dog breeds (poodle, greyhound, etc.).\n",
    "- [Regression](https://developers.google.com/machine-learning/glossary/#regression-model) problems are those that try to predict a [continuous number](https://developers.google.com/machine-learning/glossary/#numerical-data).\n",
    "    - i.e., beans in a jar, house prices.\n",
    "\n",
    "Below is an example of classification (separate blue circles from purple crosses) and regression (predict a numerical value from the data).\n",
    "\n",
    "![classification_vs_regression.png](images/classification_vs_regression.png)  \n",
    "\n",
    "*[Image source](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8ff51-4a1d-48e1-96d6-ec4c96349a4d",
   "metadata": {},
   "source": [
    "### [Training, validation, and test splits](https://youtu.be/1waHlpKiNyY)\n",
    "\n",
    "The data is normally split into training, validation, and test sets.\n",
    "\n",
    "- The [training set](https://developers.google.com/machine-learning/glossary/#training_set) is for training the model.\n",
    "- The [validation set](https://developers.google.com/machine-learning/glossary/#validation_set) (optional) is for iteratively optimising the model during training.\n",
    "- The [test set](https://developers.google.com/machine-learning/glossary/#test-set) is _only_ for testing the model at the end.\n",
    "    - This should remain untouched (i.e., [held out](https://developers.google.com/machine-learning/glossary/#holdout-data) of training).\n",
    "    - _Single-use_ (to ensure representative of future data).\n",
    "    - Think of the this like the exam at the end of a course. You don't want the students to just parrot back the teaching material. You'd like them to demonstrate understanding.\n",
    "\n",
    "![train-val-test-split.png](images/train-val-test-split.png)  \n",
    "\n",
    "*[Image source](https://stackoverflow.com/a/56100053/6250873)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c85c5-47ff-4026-8821-7cbb6655771e",
   "metadata": {},
   "source": [
    "The [size of the split](https://youtu.be/_Fe5kKmFieg) depends on the size of the dataset and the signal you're trying to predict (i.e., the smaller the signal, then the larger the test set needs to be).\n",
    "\n",
    "For example:  \n",
    "\n",
    "| Data set size | Training split (%) | Validation split (%) | Test split (%) |\n",
    "| --- | --- | --- | --- |\n",
    "| Small | 60 | 20 | 20 |\n",
    "| Medium | 80 | 10 | 10 |\n",
    "| Large | 90 | 5 | 5 |\n",
    "| Very large | 98 | 1 | 1 |\n",
    "\n",
    "The split may benefit from being stratified (preserving original class frequencies) to ensure that each set has a sample of the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575d94e-ed36-4330-94eb-9a398a9c04a4",
   "metadata": {},
   "source": [
    "### [Cross-validation](https://developers.google.com/machine-learning/glossary/#cross-validation)\n",
    "\n",
    "Cross-validation estimates how well a model generalises to new data _before_ you check it on the _single-use_ test data.\n",
    "\n",
    "It estimates the _variability_ in the _training_ score.\n",
    "\n",
    "This repeats the _training/validation_ split multiple times (_the test data remains untouched_).\n",
    "\n",
    "There are various methods for cross-validation.\n",
    "\n",
    "These are mainly variations of K-fold cross-validation, where you split the data up K times (e.g., 5).\n",
    "\n",
    "Variations then consider stratifying, shuffling, sampling, and replacing.\n",
    "\n",
    "Below is an example for 5-fold cross-validation (i.e., splitting 5 times).\n",
    "\n",
    "![cross_validation.png](images/cross_validation_diagram.png)  \n",
    "\n",
    "*[Image source](https://inria.github.io/scikit-learn-mooc/python_scripts/02_numerical_pipeline_cross_validation.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdc45f-a498-43cd-95fc-c2bc576c4594",
   "metadata": {},
   "source": [
    "(hyperparameters)=\n",
    "### [Hyperparameters](https://youtu.be/VTE2KlfoO3Q)\n",
    "\n",
    "These are what _you set before_ model training.\n",
    "\n",
    "They control the learning process.\n",
    "\n",
    "These include, for example:\n",
    "\n",
    "- The number of layers.\n",
    "- The number of units per layer.\n",
    "- The activation function(s).\n",
    "- Whether to use dropout.\n",
    "- The optimiser learning rate.\n",
    "- The batch size.\n",
    "\n",
    "They are often found through iteratively trying out different options.\n",
    "\n",
    "This iterative tuning method can be:\n",
    "\n",
    "- Systematically over a grid (i.e., grid-search).\n",
    "    - Thorough, but slow. Hence, not suitable for problems with many variables.\n",
    "- Randomly over a grid (i.e., random grid-search).\n",
    "    - Faster and more suitable for problems with many variables.\n",
    "- Other options including:\n",
    "    - Using Bayes Theorem (i.e., Bayes grid-search) to choose a new set of hyperparameters to test based on the performance of the prior set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa8ea7-c9ed-45b2-8415-86cbe1a0d5af",
   "metadata": {},
   "source": [
    "### [Parameters](https://developers.google.com/machine-learning/glossary/#parameter)\n",
    "\n",
    "These are what the model learns _during training_ (i.e., the weights / biases / coefficients of the model).\n",
    "\n",
    "The weights first need to be [initialised](https://youtu.be/s2coXdufOzE) e.g., as zeros, random numbers, etc.\n",
    "\n",
    "The parameters are then optimised in training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaea6b2-7dde-42c9-bf8e-69894bd9537a",
   "metadata": {},
   "source": [
    "### [Model](https://developers.google.com/machine-learning/glossary/#model)\n",
    "\n",
    "A model is the machine learning system.\n",
    "\n",
    "This includes the architechture, parameters, and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d96dca-e431-4452-a7ed-6bc9a701c616",
   "metadata": {},
   "source": [
    "### [Training](https://developers.google.com/machine-learning/glossary/#training)\n",
    "\n",
    "Training is the process of finding the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f0c0d-53da-472e-921b-99440ae9aa99",
   "metadata": {},
   "source": [
    "```{image} images/unteachable.jpg\n",
    ":height: 300px\n",
    ":name: unteachable.jpg\n",
    "```\n",
    "\n",
    "*[Image source](https://vas3k.com/blog/machine_learning/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e75f0-fee6-4175-b4b8-45a664409108",
   "metadata": {},
   "source": [
    "#### [Loss Function](https://developers.google.com/machine-learning/glossary/#loss)\n",
    "\n",
    "The loss function measures how accurate the model is during training.\n",
    "\n",
    "This is measured as the error on single training example. \n",
    "\n",
    "You always want to minimise the loss function.\n",
    "\n",
    "A similar concept is the [Cost Function](https://youtu.be/SHEPb1JHw5o), which is the average of the loss functions over the whole training set.\n",
    "\n",
    "The loss function is a proxy of the metric (covered below) with a smooth gradient. Note, that in some cases it is actually the same as the metric e.g., mean squared error.\n",
    "\n",
    "Common loss functions are:\n",
    "\n",
    "- [Mean squared error](https://developers.google.com/machine-learning/glossary/#mean-squared-error-mse)\n",
    "    - The average squared loss per example.\n",
    "- [Crossentropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)\n",
    "    - A measure of the difference between two probability distributions.\n",
    "    - Categorical Crossentropy for binary classification.\n",
    "    - Sparse Categorical Crossentropy for multi-class classification.\n",
    "    - Similar to the Negative Log Loss, except that this takes in _log_ probabilities, instead of raw ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2998eb84-fd10-4a3b-b4c7-51607a3bf70a",
   "metadata": {},
   "source": [
    "#### [Gradient descent](https://youtu.be/uJryes5Vk1o)\n",
    "\n",
    "A group of methods to minimise the loss function.\n",
    "\n",
    "It is how the model gets updated based on the data it sees.\n",
    "\n",
    "One step of gradient descent includes:\n",
    "\n",
    "- Forward propagate the inputs through the model to calculate the outputs of each neuron.\n",
    "- Calculate the loss (error) and gradient of the loss for these parameters.\n",
    "- [Back propagate](https://developers.google.com/machine-learning/glossary/#backpropagation) these gradients back through the model to update the parameters and reduce the loss.\n",
    "\n",
    "The gradient of the loss is reduced (optimised) in this process (i.e., the gradient descends).\n",
    "\n",
    "It aims to find the best parameters (i.e., the weights and biases that minimise the loss).\n",
    "\n",
    "The best parameters represent the single _global minimum_ of the loss (i.e., think of the lowest point in a bowl).\n",
    "\n",
    "If there are many peaks and valleys in the loss function, then there may be many _local minimums_ (i.e., where the loss function can't reduce anymore locally).\n",
    "\n",
    "Common methods of gradient descent are:\n",
    "\n",
    "- [Stochastic Gradient Descent (SGD)](https://developers.google.com/machine-learning/glossary/#stochastic-gradient-descent-sgd).\n",
    "    - Uses 1 training example per iteration.\n",
    "- [Batch gradient descent](https://youtu.be/KKfZLXcF-aE).\n",
    "    - Uses all training examples per iteration.\n",
    "- [Mini-batch gradient descent](https://youtu.be/4qJaSmvhxi8).\n",
    "    - Uses a smaller batch (e.g., 32) of training examples per iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e060c94-88c1-4cc9-a5cf-5481f5641ddf",
   "metadata": {},
   "source": [
    "#### [Optimiser](https://developers.google.com/machine-learning/glossary/#optimizer)\n",
    "\n",
    "The optimiser is the type of gradient descent used.\n",
    "\n",
    "A common choice is the [Adam (ADAptive with Momentum) optimiser](https://youtu.be/JXQT_vxqwIs). \n",
    "\n",
    "Adam combines [momentum](https://youtu.be/k8fTYJPd3_I) and [RMSprop](https://youtu.be/_e-LFe_igno) (Root Mean Squared propagation).\n",
    "\n",
    "Momentum remembers past gradients to speed up learning and get out of local minimuns.\n",
    "\n",
    "RMSprop speeds up learning in a specific direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce269f9-07be-4288-bdcd-dd9fd6e39c5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### [Metric](https://developers.google.com/machine-learning/glossary/#metric)\n",
    "\n",
    "The goal of machine learning is predicting new data.\n",
    "\n",
    "Hence, the [objective](https://developers.google.com/machine-learning/glossary/#objective) is to minimise the _test error_ (as this represents new data).\n",
    "\n",
    "This is the evaluation metric i.e., the number you primarily care about. \n",
    "\n",
    "It is helpful to have a [single evaluation metric](https://youtu.be/sofffBNhVSo) to guide decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4813f611-078f-4c96-a923-49ff5bba8f49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### [Error analysis](https://youtu.be/JoAxZsdw_3w)\n",
    "\n",
    "This is where you manually analyse the prediction errors from the model to help guide how to improve the model.\n",
    "\n",
    "An example is a [confusion matrix](https://developers.google.com/machine-learning/glossary/#confusion-matrix). This is where you aggregate a classifcation model's correct and incorrect guesses. This is useful to see what classes have more errors.\n",
    "\n",
    "For example, you could have a classification model to predict whether or not there is a tumor in the image:\n",
    "\n",
    "|  | Tumor (predicted) | Non-Tumor (predicted) |\n",
    "| --- | --- | --- |\n",
    "| Tumor (ground truth) | 18 | 1 |\n",
    "| Non-Tumor (ground truth) | 6 | 452 |\n",
    "\n",
    "So, here there are 19 ground truth images that had tumors (18 + 1), of which the model predicted 18 correct (_true positives, TP_) and 1 wrong (_false negative, FN_).\n",
    "\n",
    "Also, there are 458 ground truth images that did not have tumors (452 + 6), of which the model predicted 452 correct (_true negatives, TN_) and 6 wrong (_false positives, FP_).\n",
    "\n",
    "The _precision_ identifies the frequency of correct predictions for positive cases. Here:  \n",
    "\n",
    "$precision = TP / (TP + FP)$  \n",
    "$precision = 18 / (18 + 6)$  \n",
    "$precision = 0.75$  \n",
    "\n",
    "_Recall_ represents: out of all the possible positive labels, how many did the model correctly identify. Here:\n",
    "\n",
    "$recall = TP / (TP + FN)$  \n",
    "$recall = 18 / (18 + 1)$  \n",
    "$recall = 0.95$  \n",
    "\n",
    "\n",
    "There is often a trade-off between precision and recall (i.e., one goes up and the other goes down)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6906e8-bf84-440d-9a2b-c6f77bc7dc87",
   "metadata": {},
   "source": [
    "### [Underfit](https://youtu.be/SjQyLhQIXSM)\n",
    "\n",
    "A model _underfits_ the data when it has _high bias_ (i.e., systematic errors). \n",
    "\n",
    "This means the model is _too simple_ to capture the association (i.e., it doesn't have enough capacity to learn the generalisation).\n",
    "\n",
    "You can tell that the model underfits because there are _both_ high training errors and high test errors.\n",
    "\n",
    "To reduce underfitting, try:\n",
    "\n",
    "- Adding more features.\n",
    "- Adding more complex features.\n",
    "- Decreasing [regularisation](https://youtu.be/6g0t3Phly2M) (i.e., decrease the preference for simpler functions).\n",
    "\n",
    "_More training data is unlikely to help a model that underfits the data._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5b5ce0-3c5a-4107-8dd0-1376e1e370d9",
   "metadata": {},
   "source": [
    "(overfit)=\n",
    "### [Overfit](https://youtu.be/u73PU6Qwl1I)\n",
    "\n",
    "A model _overfits_ the data when it has _high variance_ (i.e., varies a lot). \n",
    "\n",
    "This means the model is _too complex_ to capture the association (i.e., it has too much capacity, so the training data is memorised).\n",
    "\n",
    "You can tell that the model overfits because there are _low_ training errors _but_ high test errors (i.e., there is a big difference between these errors, where the model doesn't work well on new data because it overfitted to the noise in the training data).\n",
    "\n",
    "To reduce overfitting, try:\n",
    "\n",
    "- Adding more data.\n",
    "- Using fewer or simpler features.\n",
    "- Increasing [regularisation](https://youtu.be/6g0t3Phly2M) (i.e., increase the preference for simpler functions).\n",
    "    - [L1](https://developers.google.com/machine-learning/glossary/#l1-regularization) regularlisation penalises weights in proportion to the _sum_ of their absolute values.\n",
    "    - [L2](https://youtu.be/6g0t3Phly2M) regularlisation penalises weights in proportion to the _square_ of their absolute values.\n",
    "    - [Dropout](https://youtu.be/D8PJAL-MZv8) regularlisation removes a random selection of neurons for a training step.\n",
    "- A smaller neural network with fewer layers/parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca47611-8328-44bc-94e9-15681ac361df",
   "metadata": {},
   "source": [
    "Below is an example of underfitting (linear line through non-linear data) and overfitting (very-high order polynomial passing through every training point).\n",
    "\n",
    "![underfit_vs_overfit.png](images/underfit_vs_overfit.png)  \n",
    "\n",
    "*[Image source](https://www.educative.io/edpresso/overfitting-and-underfitting)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7146e-bc2d-4078-b65e-b6b983d588e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74e521-e75b-4684-a8b3-b01951a40340",
   "metadata": {},
   "source": [
    "```{admonition} Question 1\n",
    "\n",
    "What does _deep_ mean in deep learning?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cb011c-02fa-49ee-bc84-28acf607795e",
   "metadata": {},
   "source": [
    "```{admonition} Question 2\n",
    "\n",
    "Activation functions help neural networks learn complex functions because they are:\n",
    "\n",
    "- Linear\n",
    "- Non-linear\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d82fb1-330f-425e-b7c4-fa7d17cdc116",
   "metadata": {},
   "source": [
    "```{admonition} Question 3\n",
    "\n",
    "What is a tensor?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d842b-c1f0-40fa-9fdc-7d7b997d8932",
   "metadata": {},
   "source": [
    "```{admonition} Question 4\n",
    "\n",
    "I have labelled pictures of cats and dogs that I'd like a model to classify.\n",
    "\n",
    "Is this a supervised or unsupervised problem?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4795bcf9-2c4a-415e-a2cc-6e5fde714838",
   "metadata": {},
   "source": [
    "```{admonition} Question 5\n",
    "\n",
    "I'd like a model to predict house prices from their features.\n",
    "\n",
    "Is this a classification or regression problem?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8c101-2e0e-4c91-9378-c2d3f166f9b0",
   "metadata": {},
   "source": [
    "```{admonition} Question 6\n",
    "\n",
    "How many times can I use the test data?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f86e76-408f-4dc2-adcd-d404d110bd31",
   "metadata": {},
   "source": [
    "```{admonition} Question 7\n",
    "\n",
    "I've decided on the number of hidden layers to use in my neural network.\n",
    "\n",
    "Is this a parameter or hyperparameter?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d5750e-590a-4ce6-be25-8ca1cf133d7b",
   "metadata": {},
   "source": [
    "```{admonition} Question 8\n",
    "\n",
    "Do I want to minimise or maximise the loss?\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194ee4e-5c9b-476c-8cc3-ce7f8d7f8db5",
   "metadata": {},
   "source": [
    "```{admonition} Question 9\n",
    "\n",
    "A model underfits the data when it has:\n",
    "\n",
    "- High bias\n",
    "- High variance\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd96a62-78a5-47b4-941f-7f436ee12186",
   "metadata": {},
   "source": [
    "```{admonition} Question 10\n",
    "\n",
    "If my model underfits, what might help:\n",
    "\n",
    "- Adding more features\n",
    "- Adding more data\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871b698e-4d19-4f3f-b811-17d6a5f613e2",
   "metadata": {},
   "source": [
    "```{admonition} Question 11\n",
    "\n",
    "If my model overfits, what might help:\n",
    "\n",
    "- Adding more complex features\n",
    "- Increasing regularlisation\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8af3b-5d9a-4e2c-9bd0-223837749e07",
   "metadata": {},
   "source": [
    "## {ref}`Solutions <fundamentals>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85d337-9ea3-4511-ae33-4fb036418318",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de920e-c4c1-4c47-8a80-b7f1ab648754",
   "metadata": {},
   "source": [
    "```{important}\n",
    "\n",
    "- [x] _Machine learning and deep learning are a range of prediction methods that learn associations from training data._\n",
    "- [x] _The objective is for the models to generalise to new data._\n",
    "- [x] _They mainly use tensors (multi-dimensional arrays) as inputs._\n",
    "- [x] _Problems are mainly either supervised (if you provide labels) or unsupervised (if you don't provide labels)._\n",
    "- [x] _Problems are either classification (if you're trying to predict a discrete category) or regression (if you're trying to predict a continuous number)._\n",
    "- [x] _Data is split into training, validation, and test sets._\n",
    "- [x] _The models only learn from the training data._\n",
    "- [x] _The test set is used only once._\n",
    "- [x] _Hyperparameters are set before model training._\n",
    "- [x] _Parameters (i.e., the weights and biases) are learnt during model training._\n",
    "- [x] _The aim is to minimise the loss function._\n",
    "- [x] _The model underfits when it has high bias._\n",
    "- [x] _The model overfits when it has high variance._\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970bc04-88fc-4cf2-89b3-7b4cdc8bc1f8",
   "metadata": {},
   "source": [
    "## Further information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b8126-a770-470e-af61-023eea1593fa",
   "metadata": {},
   "source": [
    "### Good practices\n",
    "\n",
    "- Start simple.\n",
    "- Incrementally test ideas.\n",
    "- The choice of algortihm depends on the problem/data (i.e., whether you use linear regression, deep learning, etc.).\n",
    "    - What assumptions are appropriate?\n",
    "- Future data should be from the same distribution as the training data (to avoid _data drift_).\n",
    "- The test set should be representative of the future data you're trying to predict. For example:\n",
    "    - For time series, test data may be 2021, while training data was 2015-2020. \n",
    "    - For medical application, test data may be completely new patients, not multiple visits from same patients in training data.\n",
    "- Consider ways to reduce the dimensionality of the data (e.g., using PCA, Principle Component Analysis).\n",
    "- Have a baseline to compare the model skill against (i.e., simple model, human performance, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446b2ff-d818-4880-a89e-a6eac6a900f6",
   "metadata": {},
   "source": [
    "### Caveats\n",
    "\n",
    "- Predictions are primarily based on associations, not explanations or causation.\n",
    "- Predictions and models are specific to the data they were trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c5a0f-2f82-441c-9097-32417fff0152",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "**Bold** are highly-recommended.\n",
    "\n",
    "- **[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/), Aurélien Géron, 2019, O’Reilly Media, Inc.**  \n",
    "    - **[Jupyter notebooks](https://github.com/ageron/handson-ml2).**  \n",
    "- [Deep Learning with Python, 2nd Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff), François Chollet, 2021, Manning.  \n",
    "    - [Jupyter notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks).  \n",
    "- [Artificial Intelligence: A Modern Approach, 4th edition](http://aima.cs.berkeley.edu/), Stuart Russell and Peter Norvig, 2021, Pearson.  \n",
    "- [Machine Learning Yearning](https://www.deeplearning.ai/programs/), Andrew Ng.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be3f20-4fb0-47c3-9b96-0a2f29dfd261",
   "metadata": {},
   "source": [
    "(online_courses)=\n",
    "### Online courses\n",
    "\n",
    "**Bold** are highly-recommended.\n",
    "\n",
    "#### Machine learning\n",
    "\n",
    "- **[Machine learning](https://www.coursera.org/learn/machine-learning), Coursera, Andrew Ng.**\n",
    "    - **CS229, Stanford University: [Video lectures](https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU).**  \n",
    "- **[Machine Learning for Intelligent Systems](http://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/), Kilian Weinberger, 2018.**  \n",
    "    - **CS4780, Cornell: [Video lectures](https://youtube.com/playlist?list=PLl8OlHZGYOQ7bkVbuRthEsaLr7bONzbXS).**  \n",
    "- [Artificial Intelligence: Principles and Techniques](https://www.youtube.com/playlist?list=PLoROMvodv4rO1NB9TD4iUZ3qghGEGtqNX), Percy Liang and Dorsa Sadigh, CS221, Standord, 2019.  \n",
    "- [Machine learning in Python with scikit-learn](https://www.fun-mooc.fr/en/courses/machine-learning-python-scikit-learn/), scikit-learn developers, 2022.\n",
    "  - [Course materials](https://inria.github.io/scikit-learn-mooc/)\n",
    "  - [Jupyter Notebooks](https://github.com/INRIA/scikit-learn-mooc/) \n",
    "\n",
    "\n",
    "#### Deep learning\n",
    "\n",
    "- **[Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning), Coursera, DeepLearning.AI (_NumPy, Keras, TensorFlow_)**\n",
    "    - **CS230, Stanford University: [Video lectures](https://www.youtube.com/playlist?list=PLoROMvodv4rOABXSygHTsbvUz4G_YQhOb), [Syllabus](http://cs230.stanford.edu/syllabus/)**\n",
    "- [NYU Deep Learning](https://atcold.github.io/NYU-DLSP21/), Yann LeCun and Alfredo Canziani, NYU, 2021 (_PyTorch_)\n",
    "    - [Video lectures](https://www.youtube.com/playlist?list=PLLHTzKZzVU9e6xUfG10TkTWApKSZCzuBI)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1ffad78e3b53a26aeabe29bd69865e9fcde2eed64638bf28084d4e5d53534f3"
  },
  "kernelspec": {
   "display_name": "swd8_intro_ml",
   "language": "python",
   "name": "swd8_intro_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
