{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f167c0-52a8-4800-9999-0419acc312a8",
   "metadata": {},
   "source": [
    "# Distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae8044-47c9-4c48-b50d-e3dadb441757",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukeconibear/intro_ml/blob/main/docs/05_distributed.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb78754a-ddf7-4ccd-a1a2-18a055f97adc",
   "metadata": {},
   "source": [
    "```{note}\n",
    "If youâ€™re in COLAB or have a local CUDA GPU, you can follow along with the more computationally intensive training in this lesson.\n",
    "\n",
    "For those in COLAB, ensure the session is using a GPU by going to: Runtime > Change runtime type > Hardware accelerator = GPU.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aef6fc-d7fe-45a5-9593-f9092ae02ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're using colab, then install the required modules\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8aa72b-89d2-47e8-9dae-d6eff282617a",
   "metadata": {},
   "source": [
    "Distributing training over multiple devices generally uses either:\n",
    "\n",
    "- [Data parallelism](https://developers.google.com/machine-learning/glossary/#data-parallelism)\n",
    "    - _Split **data** over multiple devices._\n",
    "    - Single model copied to multiple devices.\n",
    "    - Useful for big data.\n",
    "- [Model parallelism](https://developers.google.com/machine-learning/glossary/#model-parallelism)\n",
    "    - _Split **model** over multiple devices._\n",
    "    - Single data copied to multiple devices.\n",
    "    - Useful for big models (for some architectures).\n",
    "    \n",
    "This lesson focuses on data parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f972d1-8eac-424a-b800-19a6c1f6ef04",
   "metadata": {},
   "source": [
    "## [Ray Train](https://docs.ray.io/en/latest/train/train.html)\n",
    "\n",
    "Ray Train is a useful tool for distributed deep learning training for [TensorFlow (Keras)](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras) and [PyTorch](https://pytorch.org/tutorials/beginner/dist_overview.html).\n",
    "\n",
    "It handles the set up for you (e.g., [`TF_CONFIG`](https://www.tensorflow.org/guide/distributed_training#setting_up_the_tf_config_environment_variable) in TensorFlow).\n",
    "\n",
    "There are a range of examples [here](https://docs.ray.io/en/latest/train/examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03524baa-bf2a-4817-a0df-08b03c2d2142",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Note, Ray doesn't currently work on POWER9 machines e.g., Bede. See, [GitHub issue](https://github.com/ray-project/ray/issues/7476).\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8864cbb8-4483-4baa-99ea-ae49a58fa6db",
   "metadata": {},
   "source": [
    "### Example: [TensorFlow (Keras) MNIST](https://docs.ray.io/en/latest/train/examples/tensorflow_mnist_example.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401135c-c7be-4ef6-83ce-4476dc9dd40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "from ray.train import Trainer\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340dd1ed-38e0-455c-9f2b-08445bf302a4",
   "metadata": {},
   "source": [
    "#### [Define callback for reporting](https://docs.ray.io/en/latest/train/user_guide.html#logging-monitoring-and-callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe996158-18c1-4d96-8d6a-f57fd9a46110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainReportCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        ray.train.report(**logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e6947-55dd-4e48-a550-4213b4e16bc9",
   "metadata": {},
   "source": [
    "#### Set up the dataset and model\n",
    "\n",
    "The dataset will be split (sharded) across the workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef5e61-5392-4edd-bdaf-fec50b2752cf",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "The default [auto-sharding](https://www.tensorflow.org/tutorials/distribute/input#sharding) by `FILE` can cause warning messages if the data is in one file. Instead, you can specify to auto-shard by data using: `tf.data.experimental.AutoShardPolicy.DATA` (which it will fall back to anyway).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b1ba8c-03a4-434e-9409-8ff0e17246e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(batch_size):\n",
    "    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "    # The `x` arrays are in uint8 and have values in the [0, 255] range.\n",
    "    # You need to convert them to float32 with values in the [0, 1] range.\n",
    "    x_train = x_train / np.float32(255)\n",
    "    y_train = y_train.astype(np.int64)\n",
    "    ds_train = (\n",
    "        tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "        .shuffle(60000)\n",
    "        .repeat()\n",
    "        .batch(batch_size)\n",
    "    )\n",
    "\n",
    "    options = tf.data.Options()\n",
    "    options.experimental_distribute.auto_shard_policy = (\n",
    "        tf.data.experimental.AutoShardPolicy.DATA\n",
    "    )\n",
    "    ds_train = ds_train.with_options(options)\n",
    "\n",
    "    return ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cc516c-9738-4397-97ae-309a568564e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_compile_cnn_model(config):\n",
    "    learning_rate = config.get(\"lr\", 0.001)\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=(28, 28)),\n",
    "            tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "            tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(10),\n",
    "        ]\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61991c2f-cf4b-4ef2-943d-bf22e3d40ba2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Set up the training function for a _single_ worker\n",
    "\n",
    "You can [configure training](https://docs.ray.io/en/latest/train/user_guide.html#configuring-training) using the `config` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3ca5a-badd-49b6-8cff-5b8c29c62c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    batch_size = 64\n",
    "    single_worker_dataset = mnist_dataset(batch_size)\n",
    "    single_worker_model = build_and_compile_cnn_model(config)\n",
    "    single_worker_model.fit(\n",
    "        single_worker_dataset,\n",
    "        epochs=config[\"epochs\"],\n",
    "        steps_per_epoch=70,\n",
    "        verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f7875-5432-4ac1-ab40-240b72d95310",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"epochs\": 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb09cc-1b1a-4884-9d4f-026c0115816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_func(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea930e9-6620-4e67-903a-52c3f76da168",
   "metadata": {},
   "source": [
    "#### [Update training function](https://docs.ray.io/en/latest/train/user_guide.html#update-training-function)\n",
    "\n",
    "1. Set the _global_ batch size\n",
    "    - Each worker will process the same size batch as in the single-worker code.\n",
    "2. Choose your TensorFlow distributed training strategy.\n",
    "    - In this example we use the [MultiWorkerMirroredStrategy](https://www.tensorflow.org/guide/distributed_training#multiworkermirroredstrategy) for synchronous training of multiple workers across many machines.\n",
    "        - For multiple workers on _one_ machine, use [MirroredStrategy](https://www.tensorflow.org/api_docs/python/tf/distribute/MirroredStrategy).\n",
    "        - In general, the mirrored strategy mirrors the parameters across the workers, ensuring replicas are identical.\n",
    "    - Within the strategy scope context manager, you build and compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8604122-4b78-4459-a49e-0716854003fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    per_worker_batch_size = config.get(\"batch_size\", 64)\n",
    "    epochs = config.get(\"epochs\", 3)\n",
    "    steps_per_epoch = config.get(\"steps_per_epoch\", 70)\n",
    "\n",
    "    tf_config = json.loads(os.environ[\"TF_CONFIG\"])\n",
    "    num_workers = len(tf_config[\"cluster\"][\"worker\"])\n",
    "\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "    global_batch_size = per_worker_batch_size * num_workers\n",
    "    multi_worker_dataset = mnist_dataset(global_batch_size)\n",
    "\n",
    "    with strategy.scope():\n",
    "        # model building/compiling need to be within strategy.scope()\n",
    "        multi_worker_model = build_and_compile_cnn_model(config)\n",
    "\n",
    "    history = multi_worker_model.fit(\n",
    "        multi_worker_dataset,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=[TrainReportCallback()],\n",
    "        verbose=False,\n",
    "    )\n",
    "    results = history.history\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fab04c-ae80-415d-9f41-de20b03b65ae",
   "metadata": {},
   "source": [
    "#### [Create Ray Train Trainer](https://docs.ray.io/en/latest/train/user_guide.html#create-ray-train-trainer)\n",
    "\n",
    "The `Trainer` manages state and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919384c-3a06-4cc5-a733-e98257473ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tensorflow_mnist(num_workers=1, use_gpu=False, epochs=4):\n",
    "    trainer = Trainer(backend=\"tensorflow\", num_workers=num_workers, use_gpu=use_gpu)\n",
    "    trainer.start()\n",
    "    results = trainer.run(\n",
    "        train_func=train_func, config={\"lr\": 1e-3, \"batch_size\": 64, \"epochs\": epochs}\n",
    "    )\n",
    "    trainer.shutdown()\n",
    "    print(f\"Results: {results[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6ad38-736d-40d1-9025-655f838509a5",
   "metadata": {},
   "source": [
    "#### [Run the training](https://docs.ray.io/en/latest/train/user_guide.html#run-training-function)\n",
    "\n",
    "Initialise and shutdown the Ray client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c478acf-9517-4cef-91a5-d26506e7b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fce682-d5af-495b-b2da-bed0feefefff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu\n",
    "# train_tensorflow_mnist()\n",
    "\n",
    "# gpu\n",
    "# train_tensorflow_mnist(use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c511b19-86e0-4a82-8e6e-48f10f51d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d5b43-b647-484a-ae72-95ddb3ab0961",
   "metadata": {},
   "source": [
    "#### Submit the job to HPC\n",
    "\n",
    "This Python script is in full [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/tensorflow_ray_train_mnist_example.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31ed3a-e683-4990-ab93-ed849418a360",
   "metadata": {},
   "source": [
    "##### Multiple CPUs\n",
    "\n",
    "An example job submission script is (also [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/distributed_ml_on_arc4_cpu.bash)):\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#$ -cwd\n",
    "#$ -l h_rt=00:30:00\n",
    "#$ -pe smp 12\n",
    "#$ -l h_vmem=6G\n",
    "\n",
    "# activate conda and add to library path\n",
    "conda activate intro_ml\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib\n",
    "\n",
    "# run the CPU script\n",
    "python tensorflow_ray_train_mnist_example.py --num-workers 12 --epochs 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e7e19-e14e-4ef8-bad0-e7c28b6038e8",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Sometimes the `LD_LIBRARY_PATH` variable will not include the path to the conda environment, resulting it in being unable to find some libraries e.g., TensorFlow, cuDNN.\n",
    "\n",
    "To resolve this, in your script append the path to your activated conda environment to the `LD_LIBRARY_PATH` variable using:\n",
    "\n",
    "`export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib`  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae10ba3-b47a-4974-beb7-d947eec175a0",
   "metadata": {},
   "source": [
    "In this simple example using 12 CPUs, the job efficiency (using `qacct -j <JOBID>`):\n",
    "\n",
    "```\n",
    "Efficiency = 100 * cpu / (ru_wallclock * slots)\n",
    "Efficiency = 100 * 10214 / (928 * 12)\n",
    "Efficiency = 92 %\n",
    "```\n",
    "\n",
    "92% is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312974e-d089-426d-8e88-8fa47ed4cb10",
   "metadata": {},
   "source": [
    "##### Single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7f591-53aa-4861-aea5-b14994d58d9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "An example job submission script is (also [here](https://github.com/lukeconibear/intro_ml/blob/main/docs/distributed/distributed_ml_on_arc4_gpu.bash)):\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#$ -cwd -V\n",
    "#$ -l h_rt=00:15:00\n",
    "#$ -l coproc_v100=1\n",
    "\n",
    "# activate conda and add to library path\n",
    "conda activate swd8_intro_ml \n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib  # (sometimes required)\n",
    "\n",
    "# start the efficiency log for the GPU\n",
    "nvidia-smi dmon -d 5 -s um -i 0 > efficiency_log &\n",
    "\n",
    "# run the GPU script\n",
    "python tensorflow_ray_train_mnist_example.py --use-gpu True --num-workers 1 --epochs 100\n",
    "\n",
    "# stop the efficiency log\n",
    "kill %1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796288c-7942-488e-8bd3-ce0a740caef4",
   "metadata": {},
   "source": [
    "You can check the efficiency of your GPU job using the [NVIDIA System Management Interface](https://developer.nvidia.com/nvidia-system-management-interface) i.e., `nvidia-smi`.\n",
    "\n",
    "- `dmon` is for device monitoring.\n",
    "- `-d X` records every X seconds.\n",
    "- `-s um` is the metric sample, where u = utilisation and m = memory.\n",
    "- `-i 0`: device number, where 0 = first gpu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51124d83-4885-407c-b996-5f30a90373ef",
   "metadata": {},
   "source": [
    "Then you can view the results from the `efficiency_log`:\n",
    "\n",
    "```bash\n",
    "# gpu    sm   mem   enc   dec    fb  bar1\n",
    "# Idx     %     %     %     %    MB    MB\n",
    "    0     0     0     0     0     0     2\n",
    "    0     0     0     0     0     0     2\n",
    "    0     0     0     0     0     0     2\n",
    "    0     5     0     0     0   441     6\n",
    "    0     4     0     0     0 31699     6\n",
    "    0    32    14     0     0 31933     6\n",
    "    0    32    14     0     0 31933     6\n",
    "    0    17     7     0     0 31933     6\n",
    "```\n",
    "\n",
    "The main columns are `sm` (for utilisation %) and `mem` (for memory %).\n",
    "\n",
    "Here, the job utilised approximately 30% of the GPU and 15% of its memory.\n",
    "\n",
    "That's quite low, though this was for a simple MNIST example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86feb54f-8008-437d-b219-4a413f926655",
   "metadata": {},
   "source": [
    "### Example: ...\n",
    "\n",
    "More complicated one to use multiple gpus with high efficiency ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2b39c-6338-4f80-979d-b57c53c0f5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7c30a-4342-48b0-8293-0fae3a3c752d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766b007-9da0-4880-8e56-87583b5a4f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0784ac18-40c7-4928-a433-cb023c3c85a5",
   "metadata": {},
   "source": [
    "To run of multiple GPUs (e.g., 2):\n",
    "\n",
    "- Set `--num-workers 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9a255-595e-478f-a2c7-82e2dbd42541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed030ef4-2fce-4bea-a42a-1ce6df0bf19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a80f6-52bd-4f23-8dff-50bdc804381a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20cf4c2d-1031-47df-a076-863634cf360d",
   "metadata": {},
   "source": [
    "## [PyTorch (Lightning)](https://pytorch-lightning.readthedocs.io/en/stable/clouds/cluster.html)\n",
    "\n",
    "- example complex enough for multiple gpus\n",
    "- efficiency approximately 6-8% for pytorch_lightning_mnist_example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207939dc-ea9b-4471-ab63-4da571eb38cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3edba-e079-4458-86d8-5de2c5044e11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8670a50-c6ab-461b-8f45-28ca9096ea92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f754603-33b7-44ef-a1db-ce925579564a",
   "metadata": {},
   "source": [
    "To share data on a single filesystem, download the dataset once:\n",
    "\n",
    "```python\n",
    "Trainer(prepare_data_per_node=False)\n",
    "```\n",
    "\n",
    "The default behaviour is to download the data _once per node_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601f3d77-5de1-47fb-9846-1d3becf2acc6",
   "metadata": {},
   "source": [
    "[DDP Strategy](https://pytorch-lightning.readthedocs.io/en/stable/advanced/model_parallel.html#ddp-optimizations)\n",
    "\n",
    "\n",
    "```python\n",
    "# train on 8 GPUs, using the DDP strategy\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp\")\n",
    "```\n",
    "...\n",
    "\n",
    "\n",
    "```python\n",
    "# train on multiple GPUs across nodes (uses 8 GPUs in total)\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=2, num_nodes=4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d8fc19-bf0b-4a38-9691-196d166a303b",
   "metadata": {},
   "source": [
    "`num_workers`\n",
    "\n",
    "https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html#num-workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa4f0b-0a72-4b94-9d33-090cd551565b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a2155-16fe-4581-b866-8fb59329c722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba51801-638e-4079-a58b-25f948040f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4897059-38d6-4cd7-a599-e75716e70a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957c81ac-60ef-4f5f-9f1c-ae44c77543d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0054c74-183c-48d4-8beb-c920aef262e3",
   "metadata": {},
   "source": [
    "```python\n",
    "# train.py\n",
    "def main(hparams):\n",
    "    model = LightningTemplateModel(hparams)\n",
    "\n",
    "    trainer = Trainer(accelerator=\"gpu\", devices=8, num_nodes=4, strategy=\"ddp\")\n",
    "\n",
    "    trainer.fit(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    parent_parser = ArgumentParser(add_help=False)\n",
    "    hyperparams = parser.parse_args()\n",
    "\n",
    "    # TRAIN\n",
    "    main(hyperparams)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2eb480-f4ba-44a4-88a0-dee5b9993ccb",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "Some errors can show up as an [NCCL](https://developer.nvidia.com/nccl) issue.  \n",
    "\n",
    "Set the `NCCL_DEBUG=INFO` environment variable to see the actual error:  \n",
    "\n",
    "`NCCL_DEBUG=INFO python train.py`  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044acf3f-b994-4fea-b45a-ed3af02013cc",
   "metadata": {},
   "source": [
    "## Jupyter Notebook to HPC\n",
    "\n",
    "Once you've finished testing out different ideas locally in a Jupyter Notebook, you can then convert this to an exercutable script to run on a HPC.\n",
    "\n",
    "This is because the HPCs (currently accessible at Leeds, at least) are suitable for non-interactive batch jobs.\n",
    "\n",
    "The steps are to:\n",
    "\n",
    "- [Clean non-essential code](clean_nonessential_code)\n",
    "- [Refactor Jupyter Notebook code into functions](refactor_into_functions)\n",
    "- [Create a Python script](create_python_script)\n",
    "- [Create submission script](create_submission_script)\n",
    "- [(Optional) Create unit tests](create_unit_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c2064-5b2a-4e68-88fe-dad5c41e6c3d",
   "metadata": {},
   "source": [
    "(clean_nonessential_code)=\n",
    "### Clean non-essential code\n",
    "\n",
    "Some code added during the experimentation phase was only needed to test out ideas and explore the data.\n",
    "\n",
    "This non-essential code can be removed to make it more maintainable and performant.\n",
    "\n",
    "Let's use the example from the [TensorFlow Datasets MNIST example](tensorflow_datasets) we saw in Lesson 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db364bea-8762-4079-b9df-00ca02ae2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# global setup\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# download the data\n",
    "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "print(ds_train)\n",
    "print(ds_info)\n",
    "\n",
    "# preprocess the data\n",
    "def normalise_image(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "# create data pipelines\n",
    "def training_pipeline(ds_train):\n",
    "    ds_train = ds_train.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "    return ds_train\n",
    "\n",
    "\n",
    "def test_pipeline(ds_test):\n",
    "    ds_test = ds_test.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(AUTOTUNE)\n",
    "    return ds_test\n",
    "\n",
    "\n",
    "ds_train = training_pipeline(ds_train)\n",
    "ds_val = training_pipeline(ds_val)\n",
    "ds_test = test_pipeline(ds_test)\n",
    "\n",
    "\n",
    "# create the model\n",
    "inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "x = tf.keras.layers.Flatten(name=\"flatten\")(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer1\")(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer2\")(x)\n",
    "outputs = tf.keras.layers.Dense(10, name=\"outputs\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name=\"functional\")\n",
    "\n",
    "# view the model\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# plot the model accuracy\n",
    "epochs_range = range(1, NUM_EPOCHS + 1)\n",
    "\n",
    "plt.plot(epochs_range, history.history[\"accuracy\"], \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(\n",
    "    epochs_range, history.history[\"val_accuracy\"], \"b\", label=\"Validation accuracy\"\n",
    ")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0.9, 1.0])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# save the model\n",
    "path_models = f\"{os.getcwd()}/models\"\n",
    "model.save(f\"{path_models}/model_tf_mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99047dde-1025-424c-b918-a388f5d9d09f",
   "metadata": {},
   "source": [
    "Now, removing non-essential code, now the experimentation phase is complete.\n",
    "\n",
    "Here this included:\n",
    "\n",
    "- Removing downloading data information.\n",
    "- Removing unrequired print statements.\n",
    "- Viewing model summary.\n",
    "- Removing/replacing plots with text output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a8b56-6eca-43a1-92d5-407a6d617099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# global setup\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# download the data\n",
    "(ds_train, ds_val, ds_test) = tfds.load(\n",
    "    \"mnist\",\n",
    "    split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "    shuffle_files=True,\n",
    "    as_supervised=True,\n",
    "    with_info=False,\n",
    ")\n",
    "\n",
    "# preprocess the data\n",
    "def normalise_image(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "# create data pipelines\n",
    "def training_pipeline(ds_train):\n",
    "    ds_train = ds_train.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "    return ds_train\n",
    "\n",
    "\n",
    "def test_pipeline(ds_test):\n",
    "    ds_test = ds_test.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(AUTOTUNE)\n",
    "    return ds_test\n",
    "\n",
    "\n",
    "ds_train = training_pipeline(ds_train)\n",
    "ds_val = training_pipeline(ds_val)\n",
    "ds_test = test_pipeline(ds_test)\n",
    "\n",
    "\n",
    "# create the model\n",
    "inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "x = tf.keras.layers.Flatten(name=\"flatten\")(inputs)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer1\")(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer2\")(x)\n",
    "outputs = tf.keras.layers.Dense(10, name=\"outputs\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs, name=\"functional\")\n",
    "\n",
    "# compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    ")\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_val,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# view the model accuracy\n",
    "print(f\"Training accuracy: {[round(num, 2) for num in history.history['accuracy']]}\")\n",
    "print(\n",
    "    f\"Validation accuracy: {[round(num, 2) for num in history.history['val_accuracy']]}\"\n",
    ")\n",
    "\n",
    "# save the model\n",
    "path_models = f\"{os.getcwd()}/models\"\n",
    "model.save(f\"{path_models}/model_tf_mnist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c735a7-9c23-4614-8fe1-23c007f63aaf",
   "metadata": {},
   "source": [
    "(refactor_into_functions)=\n",
    "### Refactor Jupyter Notebook code into functions\n",
    "\n",
    "Now, any code that is not already in functions, can be refactored into functions.\n",
    "\n",
    "Modularising the code like this helps with diagnosing errors and creating tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f1100-48d6-4d73-aa3f-805d15293c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# global setup\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# download the data\n",
    "def download_data():\n",
    "    (ds_train, ds_val, ds_test) = tfds.load(\n",
    "        \"mnist\",\n",
    "        split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=False,\n",
    "    )\n",
    "    return ds_train, ds_val, ds_test\n",
    "\n",
    "\n",
    "# preprocess the data\n",
    "def normalise_image(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "# create data pipelines\n",
    "def training_pipeline(ds_train):\n",
    "    ds_train = ds_train.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "    return ds_train\n",
    "\n",
    "\n",
    "def test_pipeline(ds_test):\n",
    "    ds_test = ds_test.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(AUTOTUNE)\n",
    "    return ds_test\n",
    "\n",
    "\n",
    "# create and compile the model\n",
    "def create_and_compile_model():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "    x = tf.keras.layers.Flatten(name=\"flatten\")(inputs)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer1\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer2\")(x)\n",
    "    outputs = tf.keras.layers.Dense(10, name=\"outputs\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"functional\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train_model():\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "# save the model\n",
    "def save_model(model):\n",
    "    path_models = f\"{os.getcwd()}/models\"\n",
    "    model.save(f\"{path_models}/model_tf_mnist\")\n",
    "\n",
    "\n",
    "# combine the functions in a call to main\n",
    "def main():\n",
    "    ds_train, ds_val, ds_test = download_data()\n",
    "\n",
    "    ds_train = training_pipeline(ds_train)\n",
    "    ds_val = training_pipeline(ds_val)\n",
    "    ds_test = test_pipeline(ds_test)\n",
    "\n",
    "    model = create_and_compile_model()\n",
    "    history = train_model()\n",
    "    save_model(model)\n",
    "\n",
    "\n",
    "# run the functions\n",
    "main()\n",
    "\n",
    "# view the model accuracy\n",
    "print(f\"Training accuracy: {[round(num, 2) for num in history.history['accuracy']]}\")\n",
    "print(\n",
    "    f\"Validation accuracy: {[round(num, 2) for num in history.history['val_accuracy']]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8197e4-a426-4629-8f3b-b67acb1cff6a",
   "metadata": {},
   "source": [
    "(create_python_script)=\n",
    "### Create a Python script\n",
    "\n",
    "First, the call to `main()` should be placed inside a conditional invocation i.e.,:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "Now, the script can be called from a terminal by running `python script.py`.\n",
    "\n",
    "Then, the rest of the code can be convert by either:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8e7db-e3c8-4411-930c-79d08bc5db2e",
   "metadata": {},
   "source": [
    "#### Option 1 (recommended)\n",
    "\n",
    "Convert a _single cell_ to a script using the `%%writefile` IPython magic command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f0195d-937e-4847-bc35-a001a8718c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile jupyter-to-hpc_tf-mnist-example.py\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# global setup\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# download the data\n",
    "def download_data():\n",
    "    (ds_train, ds_val, ds_test) = tfds.load(\n",
    "        \"mnist\",\n",
    "        split=[\"train[:80%]\", \"train[80%:90%]\", \"train[90%:]\"],\n",
    "        shuffle_files=True,\n",
    "        as_supervised=True,\n",
    "        with_info=False,\n",
    "    )\n",
    "    return ds_train, ds_val, ds_test\n",
    "\n",
    "\n",
    "# preprocess the data\n",
    "def normalise_image(image, label):\n",
    "    return tf.cast(image, tf.float32) / 255.0, label\n",
    "\n",
    "\n",
    "# create data pipelines\n",
    "def training_pipeline(ds_train):\n",
    "    ds_train = ds_train.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_train = ds_train.cache()\n",
    "    ds_train = ds_train.shuffle(ds_info.splits[\"train\"].num_examples)\n",
    "    ds_train = ds_train.batch(128)\n",
    "    ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "    return ds_train\n",
    "\n",
    "\n",
    "def test_pipeline(ds_test):\n",
    "    ds_test = ds_test.map(normalise_image, num_parallel_calls=AUTOTUNE)\n",
    "    ds_test = ds_test.batch(128)\n",
    "    ds_test = ds_test.cache()\n",
    "    ds_test = ds_test.prefetch(AUTOTUNE)\n",
    "    return ds_test\n",
    "\n",
    "\n",
    "# create and compile the model\n",
    "def create_and_compile_model():\n",
    "    inputs = tf.keras.Input(shape=(28, 28, 1), name=\"inputs\")\n",
    "    x = tf.keras.layers.Flatten(name=\"flatten\")(inputs)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer1\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"layer2\")(x)\n",
    "    outputs = tf.keras.layers.Dense(10, name=\"outputs\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"functional\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "def train_model():\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_val,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "# save the model\n",
    "def save_model(model):\n",
    "    path_models = f\"{os.getcwd()}/models\"\n",
    "    model.save(f\"{path_models}/model_tf_mnist\")\n",
    "\n",
    "\n",
    "# combine the functions in a call to main\n",
    "def main():\n",
    "    ds_train, ds_val, ds_test = download_data()\n",
    "\n",
    "    ds_train = training_pipeline(ds_train)\n",
    "    ds_val = training_pipeline(ds_val)\n",
    "    ds_test = test_pipeline(ds_test)\n",
    "\n",
    "    model = create_and_compile_model()\n",
    "    history = train_model()\n",
    "    save_model(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run the functions\n",
    "    main()\n",
    "\n",
    "    # view the model accuracy\n",
    "    print(\n",
    "        f\"Training accuracy: {[round(num, 2) for num in history.history['accuracy']]}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Validation accuracy: {[round(num, 2) for num in history.history['val_accuracy']]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b5ebd-e3d2-4d72-87e5-f4cf2c9441e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Option 2\n",
    "\n",
    "Convert the _entire_ notebook to a script using the `nbconvert` package:\n",
    "\n",
    "```bash\n",
    "jupyter nbconvert \"my_notebook.ipynb\" --to script\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac845d9-4447-460f-b908-374b3391004f",
   "metadata": {},
   "source": [
    "#### Option 3\n",
    "\n",
    "Convert the _entire_ notebook to a script using the Jupyter menu.\n",
    "\n",
    "From within the Jupyter Notebook, click `File` > `Save and Export Notebook As ...` > `Executable Script`.\n",
    "\n",
    "This should convert the Jupyter Notebook to a `.py` file, and download it locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e145615-080f-4d65-b25d-3b8b3daf00e4",
   "metadata": {},
   "source": [
    "(create_submission_script)=\n",
    "### Create submission script\n",
    "\n",
    "For your HPC (e.g., ARC4), create the submission script.\n",
    "\n",
    "For example:\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#$ -cwd\n",
    "#$ -l h_rt=00:05:00\n",
    "#$ -l coproc_v100=1\n",
    "\n",
    "conda activate intro_ml \n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib  # (sometimes required)\n",
    "\n",
    "python jupyter-to-hpc_tf-mnist-example.py\n",
    "```\n",
    "\n",
    "Ensure both of these files are on the HPC, alongside the corresponding conda environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ea8d1-52a9-4085-9008-10d1dd60ed15",
   "metadata": {
    "tags": []
   },
   "source": [
    "(create_unit_tests)=\n",
    "### (Optional) Create unit tests\n",
    "\n",
    "It's good practise to write [tests](https://the-turing-way.netlify.app/reproducible-research/testing.html) for your code.\n",
    "\n",
    "[pytest](https://docs.pytest.org/en/6.2.x/) are [NumPy](https://numpy.org/doc/stable/reference/testing.html) are both good, commonly-used tools for testing in Python.\n",
    "\n",
    "There are various [levels, types, and methods](https://softwaretestingfundamentals.com/) of testing.\n",
    "\n",
    "For example, you could [unit test](https://softwaretestingfundamentals.com/unit-testing/) individual functions, like the downloading and splitting of data:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def test_split_data(ds_train, ds_val, ds_test):\n",
    "    num_samples_train = len(ds_train)\n",
    "    num_samples_val = len(ds_val)\n",
    "    num_samples_test = len(ds_test)\n",
    "    \n",
    "    num_samples = num_samples_train + num_samples_val + num_samples_test\n",
    "    \n",
    "    np.testing.assert_almost_equal(num_samples_train / num_samples, 0.8, decimal=3)\n",
    "    np.testing.assert_almost_equal(num_samples_val / num_samples, 0.1, decimal=3)\n",
    "    np.testing.assert_almost_equal(num_samples_test / num_samples, 0.1, decimal=3)\n",
    "    \n",
    "\n",
    "test_split_data(ds_train, ds_val, ds_test)\n",
    "```\n",
    "\n",
    "You could also perform [system testing](https://softwaretestingfundamentals.com/system-testing/). For example, checking the final validation accuracy is above a threshold:\n",
    "\n",
    "```python\n",
    "def test_final_val_accuracy_above_threshold(threshold):\n",
    "    assert history.history['val_accuracy'][-1] >= threshold\n",
    "    \n",
    "test_final_val_accuracy_above_threshold(0.96)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7146e-bc2d-4078-b65e-b6b983d588e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74e521-e75b-4684-a8b3-b01951a40340",
   "metadata": {},
   "source": [
    "```{admonition} Question 1\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8af3b-5d9a-4e2c-9bd0-223837749e07",
   "metadata": {},
   "source": [
    "## {ref}`Solutions <distributed>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85d337-9ea3-4511-ae33-4fb036418318",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de920e-c4c1-4c47-8a80-b7f1ab648754",
   "metadata": {},
   "source": [
    "```{important}\n",
    "\n",
    "- [x] _..._\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bcbb5-e4c3-4032-b7af-8fd24d9e19e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Further information\n",
    "\n",
    "### Good practices\n",
    "\n",
    "- Ensure works on a single workers first, _before_ going distributed.\n",
    "- Ensure that you need the overhead of distributing over multiple GPUs e.g., could you instead use 1 GPU and model checkpointing?\n",
    "- Ensure that the problem is complex enough to use multiple GPUs efficiently.\n",
    "- Batch the dataset with the global batch size e.g., for 8 devices each capable of a btach of 64 use the global batch size of 512 (= 8 * 64).  \n",
    "- [Distributed training on PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/advanced/model_parallel.html)\n",
    "- Performance tips from [PyTorch](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)\n",
    "\n",
    "### Other options\n",
    "\n",
    "- [Horovod](https://horovod.ai/)\n",
    "    - A library to make distributed deep learning fast and easy to use.\n",
    "- [DeepSpeed](https://www.deepspeed.ai/)\n",
    "    - A deep learning optimization library that makes distributed training easy, efficient, and effective.\n",
    "- [FairScale](https://fairscale.readthedocs.io/en/latest/)\n",
    "    - A PyTorch extension library for high performance and large scale training.\n",
    " \n",
    "### Resources\n",
    "\n",
    "- [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)\n",
    "- [scikit-learn parallelism](https://scikit-learn.org/stable/computing/parallelism.html)\n",
    "- [Convert ML experiment to production](https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-convert-ml-experiment-to-production)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1ffad78e3b53a26aeabe29bd69865e9fcde2eed64638bf28084d4e5d53534f3"
  },
  "kernelspec": {
   "display_name": "swd8_intro_ml",
   "language": "python",
   "name": "swd8_intro_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
