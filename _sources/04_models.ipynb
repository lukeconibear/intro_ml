{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f167c0-52a8-4800-9999-0419acc312a8",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae8044-47c9-4c48-b50d-e3dadb441757",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukeconibear/intro_ml/blob/main/docs/04_models.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aef6fc-d7fe-45a5-9593-f9092ae02ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're using colab, then install the required modules\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    %pip install --quiet keras-tuner --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dacb87-99ef-4c96-a352-33f033ffd575",
   "metadata": {},
   "source": [
    "```{note}\n",
    "If youâ€™re in COLAB or have a local CUDA GPU, you can follow along with the more computationally intensive training in this lesson.\n",
    "\n",
    "For those in COLAB, ensure the session is using a GPU by going to: Runtime > Change runtime type > Hardware accelerator = GPU.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8934f2-6535-4a79-9f70-7baa5febedaa",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Tune the [hyperparameters](hyperparameters) to find the best model.\n",
    "\n",
    "### TensorFlow (Keras)\n",
    "\n",
    "[KerasTuner](https://keras.io/guides/keras_tuner/getting_started/) is a library the helps you pick the best hyperparameters for your model.\n",
    "\n",
    "#### Other options\n",
    "\n",
    "- [TensorFlow Model Optimisation Toolkit](https://www.tensorflow.org/model_optimization)\n",
    "    - A suite of tools for optimising machine learning models.\n",
    "- [AutoKeras](https://autokeras.com/)\n",
    "    - An AutoML system to automate the building of the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a6cdd-4b3f-4eeb-a577-db48a6f35a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras_tuner\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398ef50-2df2-4929-912c-1a2fc8bfd3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d15e1-3927-4df1-9d70-004846c6745e",
   "metadata": {},
   "source": [
    "Build the model with the `hyperparameters` option.\n",
    "\n",
    "- The number of units.\n",
    "- The activation function to use\n",
    "- Whether to use dropout.\n",
    "- The optimiser learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b96244-4010-4ac4-979f-057a7aa64f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hyperparameters):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(\n",
    "        tf.keras.layers.Dense(\n",
    "            units=hyperparameters.Int(\"units\", min_value=32, max_value=512, step=32),\n",
    "            activation=hyperparameters.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "        )\n",
    "    )\n",
    "    if hyperparameters.Boolean(\"dropout\"):\n",
    "        model.add(tf.keras.layers.Dropout(rate=0.25))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    learning_rate = hyperparameters.Float(\n",
    "        \"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\"\n",
    "    )\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3287045-5875-4055-a50e-6786cb66cdc8",
   "metadata": {},
   "source": [
    "First, prepare a directory to store all the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b596c293-f631-42f1-be3b-75f0f07c6cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_path = f\"{os.getcwd()}/models/hyperparameters\"\n",
    "if not os.path.exists(hyperparameters_path):\n",
    "    os.makedirs(hyperparameters_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326f86da-e0da-4df0-9bb9-0772efa72504",
   "metadata": {},
   "source": [
    "Now, initialise the tuner.\n",
    "\n",
    "This uses Random Search, though you could also use [other methods](https://www.tensorflow.org/tutorials/keras/keras_tuner#instantiate_the_tuner_and_perform_hypertuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ff37b-5897-415b-ad42-5b880e152ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.RandomSearch(\n",
    "    hypermodel=build_model,  # the model building function\n",
    "    objective=\"val_accuracy\",  # the objective to optimise\n",
    "    max_trials=3,  # the number of trials for the search\n",
    "    executions_per_trial=2,  # the number of models to build and fit per trial\n",
    "    overwrite=True,  # whether to overwrite previous results\n",
    "    directory=hyperparameters_path,  # the path for the hyperparameter results\n",
    "    project_name=\"hp_example\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e6bc75-cd60-4415-913b-42a607b85a7c",
   "metadata": {},
   "source": [
    "View the search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe61c3c-56e5-4c72-a11c-6491113d18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5f16a-d68b-419c-a881-728082e21d2f",
   "metadata": {},
   "source": [
    "Load in the MNIST dataset for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3fe78-025b-41d3-838d-c37e935d3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x[:-10000]\n",
    "x_val = x[-10000:]\n",
    "y_train = y[:-10000]\n",
    "y_val = y[-10000:]\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.0\n",
    "x_val = np.expand_dims(x_val, -1).astype(\"float32\") / 255.0\n",
    "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.0\n",
    "\n",
    "num_classes = 10\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de76a9-43fb-4ad7-a6c2-26b0d3db18f1",
   "metadata": {},
   "source": [
    "Start the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d258314-6bbb-41a5-867c-99d2cf3c19d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    tuner.search(\n",
    "        x_train, y_train, epochs=2, validation_data=(x_val, y_val), verbose=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334c663-f979-42e8-a435-168054ddb9b3",
   "metadata": {},
   "source": [
    "View the results.\n",
    "\n",
    "You can also visualise them using [TensorBoard](https://keras.io/guides/keras_tuner/visualize_tuning/).results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d5885a-bc9e-4fae-b336-a717d9b26436",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14eaa8-f20c-4cca-b6c2-6e8d6ae22363",
   "metadata": {},
   "source": [
    "Select the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ffaf62-4e70-49fc-8c25-5fdfdcfa4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56160ca-711f-4669-826a-6455aed5c5b1",
   "metadata": {},
   "source": [
    "Show the best model's summary.\n",
    "\n",
    "Note, the `Sequential` model will need to be built first with a input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d98161-13e8-4e4c-9deb-7d912afb84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    best_model.build(input_shape=(None, 28, 28))\n",
    "    best_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eb1850-3e52-4b72-a9fd-f81c6521a55d",
   "metadata": {},
   "source": [
    "### PyTorch (Lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398481f-5127-4879-86b5-b6b85dde41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e929f7-8d7e-4507-88ae-2118ae0adb5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14529c8-8b0d-4c32-b437-16853010e0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae564adb-b9c3-4ccd-ae1b-9eb3057f35b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158b150-9988-41d2-92e3-2a3495d513fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a61fc1-7e07-4d0e-b7f5-04ce3cdad04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7d4346-f729-49c1-b1dd-734b1a1d1430",
   "metadata": {},
   "source": [
    "## [Transfer learning](https://youtu.be/yofjFQddwHE)\n",
    "\n",
    "Transfer learning is where a model that has been pre-trained on a problem is transferred to another similar problem. [For example](https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/), if a model learnt to classify images it can be transferred to other image classification problems.\n",
    "\n",
    "This works because the pre-trained model has _extracted features_ from the similar task (i.e., lower-level: lines, curves, etc. and higher-level: eyes, ears, etc.).\n",
    "\n",
    "Commonly, this is done via:\n",
    "\n",
    "- Instantiate a pre-trained model and load pre-trained weights into it.\n",
    "- Extract layers and freeze them (i.e., `trainable = False`) to retain their information (e.g., for lower level features).\n",
    "- Add new trainable layers on top of these frozen layers.\n",
    "- Train the new layers on your dataset.\n",
    "- Optional: Unfreeze the frozen layers and train these on the dataset with a very low learning rate (i.e., [fine tuning](https://www.tensorflow.org/tutorials/images/transfer_learning#fine_tuning)).\n",
    "    - This is done when the training dataset is larger and very similar to the original pre-trained dataset.\n",
    "\n",
    "This is useful when:\n",
    "\n",
    "- You have a small dataset.\n",
    "- You want to take advantage of huge models without the costs of training them.\n",
    "\n",
    "There are range of pre-trained models available in [Keras Applications](https://keras.io/api/applications/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec47f1b-3e61-4392-9270-6f0142d47260",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example: Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951c8ef-2f46-422a-aee4-45e5fab7ce87",
   "metadata": {},
   "source": [
    "#### [TensorFlow (Keras)](https://www.tensorflow.org/tutorials/images/transfer_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edeafd3-e8cf-4d68-bfba-52810f7e0c62",
   "metadata": {},
   "source": [
    "Download and split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03806e68-3f34-4b29-8118-e3a8949ea198",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # tfds.disable_progress_bar()\n",
    "\n",
    "    ds_train, ds_val, ds_test = tfds.load(\n",
    "        \"cats_vs_dogs\",\n",
    "        # Reserve 10% for validation and 10% for test\n",
    "        split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
    "        as_supervised=True,  # Include labels\n",
    "    )\n",
    "\n",
    "    print(\"Number of training samples: %d\" % tf.data.experimental.cardinality(ds_train))\n",
    "    print(\"Number of validation samples: %d\" % tf.data.experimental.cardinality(ds_val))\n",
    "    print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e336f5f-f69c-4acd-9467-086e38581882",
   "metadata": {},
   "source": [
    "View a few of the training samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde4786-2ea3-41e0-86f8-d01987374fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    labels = {0: \"Cat\", 1: \"Dog\"}\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, (image, label) in enumerate(ds_train.take(9)):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(image)\n",
    "        plt.title(labels[int(label)])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff6a4c6-8537-4f1f-91da-0bb94b148a4b",
   "metadata": {},
   "source": [
    "First, preprocess the data.\n",
    "\n",
    "This is to standardise the image sizes and normalise the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54f2db-4271-4543-87ed-954eef0f51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    IMAGE_SIZE = (150, 150)\n",
    "\n",
    "    ds_train = ds_train.map(lambda x, y: (tf.image.resize(x, IMAGE_SIZE), y))\n",
    "    ds_val = ds_val.map(lambda x, y: (tf.image.resize(x, IMAGE_SIZE), y))\n",
    "    ds_test = ds_test.map(lambda x, y: (tf.image.resize(x, IMAGE_SIZE), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd65dc59-20e8-4141-af3a-e30d68448fca",
   "metadata": {},
   "source": [
    "Now, we can [cache](cache_tf), [batch](batch_tf), and [prefetch](prefetch_tf) the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae43c5e-8c0b-4010-924f-bb8cad9eb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    BATCH_SIZE = 32\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    ds_train = ds_train.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "    ds_val = ds_val.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\n",
    "    ds_test = ds_test.cache().batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bad4a-4c5d-4c9a-a329-1ff8784f5f1a",
   "metadata": {},
   "source": [
    "Apply some [data augmentation](data_augmentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b71d7-80cd-436d-b7b8-52bf38b16399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97c0d2-7a41-41d2-ac05-32d3d7b12ee1",
   "metadata": {},
   "source": [
    "Now, build the model.\n",
    "\n",
    "Steps:\n",
    "\n",
    "- First, load the pre-trained model as the base.\n",
    "    - This example uses the Xception model (other options are [here](https://keras.io/api/applications/)).\n",
    "    - The weights were pre-trained on ImageNet (a large image dataset).\n",
    "    - The top layer from the Xception classifier is not included (i.e., the one that predicts the ImageNet categories).\n",
    "- Freeze the layers from the base model.\n",
    "- Add new layers on top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b4baa6-cb61-441f-a237-25b06c1685d0",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "[BatchNormalisation](https://www.tensorflow.org/tutorials/images/transfer_learning#important_note_about_batchnormalization_layers) layers should be kept in inference mode (i.e., the base model should keep `training = False`) when the base model is unfrozen for fine-tuning.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387ad510-51c9-431b-a0cf-2b0b562abd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.Xception(\n",
    "    weights=\"imagenet\",  # use weights from ImageNet\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False,  # don't include Xceptions classifier for ImageNet\n",
    ")\n",
    "base_model.trainable = False  # freeze the pre-trained model\n",
    "\n",
    "inputs = tf.keras.Input(shape=(150, 150, 3))  # create new model on top\n",
    "x = data_augmentation(inputs)  # apply data augmentation\n",
    "\n",
    "scale_layer = tf.keras.layers.Rescaling(\n",
    "    scale=1.0 / 127.5, offset=-1\n",
    ")  # rescale from [0, 255] to [-1.0, 1.0]\n",
    "x = scale_layer(x)\n",
    "\n",
    "x = base_model(\n",
    "    x, training=False\n",
    ")  # keep batchnorm layers in infernece mode (see note above)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)  # convert 2D locations to vector\n",
    "x = tf.keras.layers.Dropout(0.2)(x)  # for regularisation\n",
    "\n",
    "outputs = tf.keras.layers.Dense(1)(x)  # prediction layer\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad9714-a2d9-40c2-8093-f8a73034e0b7",
   "metadata": {},
   "source": [
    "1% of the parameters are trainable, with 99% from the pre-trained model.\n",
    "\n",
    "That will save a lot of computation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af16761-2afe-4a43-ba8f-68e82d6e9690",
   "metadata": {},
   "source": [
    "Now, train the top layers only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835984d8-b8ab-4090-8451-8fcfab4bbd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e202fae-43f0-49c5-8369-650f370a9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # just 1 epoch for the demonstration, would increase this to say 20 normally\n",
    "    NUM_EPOCHS = 1\n",
    "    model.fit(ds_train, epochs=NUM_EPOCHS, validation_data=ds_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd42cf-3b71-4c3e-acde-883887caf99f",
   "metadata": {},
   "source": [
    "Now you could perform [fine tuning](https://www.tensorflow.org/tutorials/images/transfer_learning#fine_tuning).\n",
    "\n",
    "This is where you unfreeze the frozen layers and train these on the dataset with a very low learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21b932-824f-4030-a83b-aa864ba45c44",
   "metadata": {},
   "source": [
    "#### [TensorFlow Hub](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub)\n",
    "\n",
    "There are many [pre-trained models](https://tfhub.dev/) on [TensorFlow Hub](https://www.tensorflow.org/hub/overview) for use in transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83987582-1a4e-429b-bd2e-724b6a8ff47f",
   "metadata": {},
   "source": [
    "Download the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8785e-aefb-400d-9fe8-3745efdb4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = tf.keras.utils.get_file(\n",
    "    \"flower_photos\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\",\n",
    "    untar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6e59f-46a3-4c4b-9dab-903c9370b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "\n",
    "ds_train = tf.keras.utils.image_dataset_from_directory(\n",
    "    str(data_root),\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    str(data_root),\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9f7f8-6b07-4e1c-8e8e-3bd634e7136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.array(ds_train.class_names)\n",
    "num_classes = len(class_names)\n",
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058e937-6867-4286-a5fe-c2df5c5220f0",
   "metadata": {},
   "source": [
    "Pre-process the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641d0d84-7c1c-404f-afc8-defaf3d8fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1.0 / 255)\n",
    "ds_train = ds_train.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba126a-f48c-499d-a1a0-0bb740fa6a9f",
   "metadata": {},
   "source": [
    "Create the data pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddaf8a-0768-4bba-a962-12329c74cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "ds_train = ds_train.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e158b-512b-4ef6-816c-6145e8fc251e",
   "metadata": {},
   "source": [
    "Download the headless pre-trained model (i.e., without the top layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35946629-b84f-4fb5-9ed7-8336cc973c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_v2 = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "inception_v3 = \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4\"\n",
    "\n",
    "feature_extractor_model = (\n",
    "    mobilenet_v2  # change to inception_v2, or choose a different one\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec802650-516d-4134-9c5a-d17d2d064309",
   "metadata": {},
   "source": [
    "The download location defaults to a [local temporary directory](https://www.tensorflow.org/hub/caching).\n",
    "\n",
    "To change this, set the following:\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/nobackup/username/tf_hub_modules\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd15653-35ee-4719-89f3-eaee38f645b4",
   "metadata": {},
   "source": [
    "Create the headless pre-trained _layer_ by setting trainable to False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750a242-31f6-4d0f-8f8a-e820c0626456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880b9d6-19a8-4acc-9d78-5a283830aa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_layer = hub.KerasLayer(\n",
    "    feature_extractor_model, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3), trainable=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b7258-76b6-4ffc-83e4-d5dc0048e3ca",
   "metadata": {},
   "source": [
    "Create the new model with the pre-trained layer as the base and a new classification layer on top:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3616e4d7-9949-40a7-ad43-3ee45aed5ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "    [feature_extractor_layer, tf.keras.layers.Dense(num_classes)]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f780fe-ca6d-4592-a3f9-62fa23bfb861",
   "metadata": {},
   "source": [
    "Compile the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4260010-de15-4a3e-83e0-7b6fd6961324",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439647dd-9ccb-4f60-a239-34e502fed1e9",
   "metadata": {},
   "source": [
    "Train the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd8f9be-482d-44eb-81bb-77b70ad31841",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    NUM_EPOCHS = 3\n",
    "    history = model.fit(ds_train, validation_data=val_ds, epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70362f4b-9e33-40a9-823a-65340f36bf83",
   "metadata": {},
   "source": [
    "#### [PyTorch (Lightning)](https://pytorch-lightning.readthedocs.io/en/stable/advanced/transfer_learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d7eef8-4f18-43b9-9986-314e8e5fcc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a952a51f-6a2b-4085-8845-2909ba561fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenetTransferLearning(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # init a pretrained resnet\n",
    "        backbone = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # extract some setup details\n",
    "        num_filters = backbone.fc.in_features\n",
    "        layers = list(backbone.children())[:-1]\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "\n",
    "        # use the pretrained model to classify cifar-10 (10 image classes)\n",
    "        num_target_classes = 10\n",
    "        self.classifier = nn.Linear(num_filters, num_target_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.feature_extractor.eval()\n",
    "        with torch.no_grad():\n",
    "            representations = self.feature_extractor(x).flatten(1)\n",
    "        x = self.classifier(representations)\n",
    "        # ... the rest of your LightningModule ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae015923-d503-4396-a765-7939b3b38194",
   "metadata": {},
   "source": [
    "#### [PyTorch Lightning Bolts](https://lightning-bolts.readthedocs.io/en/latest/introduction_guide.html#for-pretrained-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef4edee3-3949-414c-ae91-1d12050131f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.models.autoencoders import VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0bc8938-5636-4db5-a37b-70ed0b2342a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(input_height=32, pretrained='imagenet2012')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "330af3b2-f2b3-4ee1-ad67-ac72ff94bb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetEncoder(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): EncoderBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): EncoderBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): EncoderBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): EncoderBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): EncoderBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): EncoderBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): EncoderBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): EncoderBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = model.encoder\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ebb0f8-6e06-4a61-b4bd-5ecb3c1279e8",
   "metadata": {},
   "source": [
    "#### [PyTorch Lightning-Flash](https://lightning-flash.readthedocs.io/en/latest/general/finetuning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69c2c13-68ce-4c49-a0d3-7d0932162fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import flash\n",
    "import torch\n",
    "from flash.core.classification import LabelsOutput\n",
    "from flash.core.data.utils import download_data\n",
    "from flash.image import ImageClassificationData, ImageClassifier\n",
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0e292c-6ef8-4e94-99f1-13c7c66da400",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98787758-9f45-4795-82aa-3ed32bb18d37",
   "metadata": {},
   "source": [
    "Download and organize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad4cfea-5449-46e2-97c4-23d96c5f0db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"{os.getcwd()}/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e759db5-5552-49aa-af1b-bcacedd3c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/earlacoa/miniconda3/envs/swd8_intro_ml/lib/python3.9/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pl-flash-data.s3.amazonaws.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "download_data(\"https://pl-flash-data.s3.amazonaws.com/hymenoptera_data.zip\", f\"{data_path}/\")\n",
    "\n",
    "datamodule = ImageClassificationData.from_folders(\n",
    "    train_folder=f\"{data_path}/hymenoptera_data/train/\",\n",
    "    val_folder=f\"{data_path}/hymenoptera_data/val/\",\n",
    "    test_folder=f\"{data_path}/hymenoptera_data/test/\",\n",
    "    batch_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a95739-4b64-4827-8847-74ff36b3826c",
   "metadata": {},
   "source": [
    "Build the model using desired Task (e.g., [ResNet18](https://arxiv.org/pdf/1512.03385.pdf)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb51ab24-0e85-4843-8acd-34c6ecc97490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/earlacoa/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235c9892d11e4d9ca25eede0b5672aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/earlacoa/miniconda3/envs/swd8_intro_ml/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:244: UserWarning: Attribute 'adapter' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['adapter'])`.\n",
      "  rank_zero_warn(\n",
      "/home/earlacoa/miniconda3/envs/swd8_intro_ml/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:244: UserWarning: Attribute 'backbone' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['backbone'])`.\n",
      "  rank_zero_warn(\n",
      "/home/earlacoa/miniconda3/envs/swd8_intro_ml/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:244: UserWarning: Attribute 'head' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['head'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "model = ImageClassifier(backbone=\"resnet18\", labels=datamodule.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc70c5-dc43-41ca-9f9e-ef06d996f155",
   "metadata": {},
   "source": [
    "Create the trainer (run one epoch for demo):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c8863e-d03e-4837-9d8a-a38c5ff1f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = flash.Trainer(max_epochs=1, gpus=torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd84ae2-ac0f-4f98-8270-64cc32bd6763",
   "metadata": {},
   "source": [
    "Finetune the model.\n",
    "\n",
    "`strategy=\"freeze\"` keeps the pre-trained model (backbone) frozen throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5a23bfe-3959-4db3-902e-5f83fdd7a494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type           | Params\n",
      "-------------------------------------------------\n",
      "0 | train_metrics | ModuleDict     | 0     \n",
      "1 | val_metrics   | ModuleDict     | 0     \n",
      "2 | test_metrics  | ModuleDict     | 0     \n",
      "3 | adapter       | DefaultAdapter | 11.2 M\n",
      "-------------------------------------------------\n",
      "10.6 K    Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/earlacoa/miniconda3/envs/swd8_intro_ml/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/earlacoa/miniconda3/envs/swd8_intro_ml/lib/python3.9/site-packages/pytorch_lightning/utilities/data.py:72: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/home/earlacoa/miniconda3/envs/swd8_intro_ml/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d00f42e2ab41d2983467b92e26fead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    trainer.finetune(model, datamodule=datamodule, strategy=\"freeze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78765680-f442-4e1d-b36f-26c3884958ae",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed2faaee-883c-4fd7-8749-3a614b323311",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{os.getcwd()}/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "151c97e0-65b8-40a2-bfef-2a7ab10dc33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    trainer.save_checkpoint(f\"{model_path}/image_classification_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8d4cc-6903-42c9-9518-a0dc0dff063b",
   "metadata": {},
   "source": [
    "## [Callbacks](https://keras.io/api/callbacks/)\n",
    "\n",
    "Callbacks are objects that get called by the model at different points during training, in particular after each batch or epoch.\n",
    "\n",
    "For example, they could be used to:\n",
    "\n",
    "- Save a model version at regularly intervals or once attained a metric threshold (i.e., checkpointing).\n",
    "- Monitor and profile the training progress (i.e., TensorBoard)\n",
    "- Change the learning rate when training plateaus.\n",
    "- Fine tuning when the training plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c97c403-1c0b-4fb6-87b8-ef9fb65f3b37",
   "metadata": {},
   "source": [
    "### Checkpoints\n",
    "\n",
    "You can save a model (/ model weights) at custom checkpoints e.g., the latest best model in terms of accuracy per epoch.\n",
    "\n",
    "The `save_best_only` option is useful as after training only the best model will have been saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92909181-ec1c-49a9-b658-d48f00194822",
   "metadata": {},
   "source": [
    "#### [TensorFlow (Keras)](https://www.tensorflow.org/guide/checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647ebcff-cc7b-404d-9d9f-8cf2fde39789",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = f\"{os.getcwd()}/models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d014bc-7569-42bc-b71b-bf761e75eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tf_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=path_models + \"/model_{epoch}\",\n",
    "    save_freq=\"epoch\",  # save a model version at the end of each epoch\n",
    "    save_best_only=True,  # only save a model if val_accuracy improved\n",
    "    monitor=\"val_accuracy\",\n",
    "    # save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bb08d-4975-486a-b0e0-a90403bb8b12",
   "metadata": {},
   "source": [
    "#### PyTorch (Lightning)\n",
    "\n",
    "Checkpointing enabled by default in PyTorch Lightning.\n",
    "\n",
    "Model checkpoints are saved per epoch to `lightning_logs/version_X/checkpoints`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf9462-b57e-4c86-ba1d-2e76c53493a2",
   "metadata": {},
   "source": [
    "### Fault tolerance\n",
    "\n",
    "For longer or distributed training, it's helpful to save the model at regular intervals in case it crashes during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f2f3e2-9b77-4740-877b-2e7de312ec9b",
   "metadata": {},
   "source": [
    "#### [TensorFlow (Keras)](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#fault_tolerance)\n",
    "\n",
    "This is done via [BackupAndRestore](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras#checkpoint_saving_and_restoring) (previously was done using ModelCheckpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c472d0-72ec-40d4-a00d-7e6750519d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tf_model_backup = tf.keras.callbacks.BackupAndRestore(\n",
    "    backup_dir=f\"{path_models}/backup\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e6e46-8917-4e96-afef-38e0af7818dd",
   "metadata": {},
   "source": [
    "#### [PyTorch (Lightning)](https://pytorch-lightning.readthedocs.io/en/stable/advanced/fault_tolerant_training.html)\n",
    "\n",
    "By default, if `Trainer.fit()` fails, then it can be restarted automatically from the _beginning_ of the epoch it failed on.\n",
    "\n",
    "With Fault Tolerant Training, when `Trainer.fit()` fails in the middle of an epoch during training or validation, Lightning will restart exactly where it failed, and everything will be restored.\n",
    "\n",
    "Enabled via:\n",
    "\n",
    "```bash\n",
    "PL_FAULT_TOLERANT_TRAINING=1 python script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c31d6-aec4-469a-a244-0cc6909cf8e6",
   "metadata": {},
   "source": [
    "### Logging and Profiling\n",
    "\n",
    "[Tensorboard](https://www.tensorflow.org/tensorboard) is a browser-based application that provides live plots of loss and metrics for training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6041495d-ddd9-4d75-9772-a3e65dfc2c4d",
   "metadata": {},
   "source": [
    "#### [TensorFlow (Keras)](https://www.tensorflow.org/guide/profiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7d71d4-76a3-4084-9380-bdf14f8ee32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tf_tensorboard_with_profiling = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=f\"{path_models}/logs\",\n",
    "    profile_batch=(1, 5),  # profile batches 1 to 5\n",
    "    update_freq=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc933d-e8fa-4e24-a843-d9f22f8c6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove previous logs\n",
    "!rm -r logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a2ed4-6869-4303-91ba-cb00d63a24c8",
   "metadata": {},
   "source": [
    "View them with:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=/full_path_to_your_logs\n",
    "```\n",
    "\n",
    "Also, in-line in [Jupyter Notebooks / Google Colab](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks):\n",
    "\n",
    "```python\n",
    "from tensorboard import notebook\n",
    "notebook.list() # list open tensorboard instances\n",
    "notebook.display(port=6006, height=1000) # open tensorboard\n",
    "\n",
    "# or\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6eec1-5b30-4fb0-bd07-1648765ec149",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### PyTorch (Lightning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1aba0d-b10b-4386-95ea-860116160cb1",
   "metadata": {},
   "source": [
    "##### [Logging](https://pytorch-lightning.readthedocs.io/en/stable/extensions/logging.html)\n",
    "\n",
    "PyTorch Lightning uses TensorBoard for loggin by default.  \n",
    "\n",
    "You can view the logs using:\n",
    "\n",
    "```python\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e84c9-7f9a-4fc4-87f7-b379d05dca87",
   "metadata": {},
   "source": [
    "##### Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425c3e17-b99d-45b7-becd-2f3ec6c680f9",
   "metadata": {},
   "source": [
    "You can profile (time and memory) with [PyTorch](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) using the `torch.autograd.profiler` context manager:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6fc769f-d4ad-45a4-8e57-43ff74ed1f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.autograd.profiler as profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f6999d4-4411-47f3-9ab9-ffea8d342033",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()\n",
    "inputs = torch.randn(5, 3, 244, 244)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28a15bbc-51d1-4411-97ce-04394832fefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with profiler.profile() as prof:\n",
    "    with profiler.record_function(\"model_inference\"):\n",
    "        model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821bce0e-056e-428d-b759-b72a734f190b",
   "metadata": {},
   "source": [
    "This can then help you find bottlenecks in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e643cb1-695d-4f26-8ec6-121f5e239fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  model_inference         4.70%       7.858ms        99.99%     167.127ms     167.127ms             1  \n",
      "                     aten::conv2d         0.05%      91.000us        74.49%     124.503ms       6.225ms            20  \n",
      "                aten::convolution         0.19%     311.000us        74.43%     124.412ms       6.221ms            20  \n",
      "               aten::_convolution         0.11%     189.000us        74.25%     124.101ms       6.205ms            20  \n",
      "         aten::mkldnn_convolution        73.99%     123.673ms        74.13%     123.912ms       6.196ms            20  \n",
      "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 167.148ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889a8689-0979-419c-b34a-2f9b3e7952ee",
   "metadata": {},
   "source": [
    "In [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/stable/advanced/profiler.html), for a simple profiler over method calls and time spent calling them:\n",
    "\n",
    "```python\n",
    "Trainer(profiler=True)\n",
    "```\n",
    "\n",
    "For more control, use:\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.profiler import AdvancedProfiler\n",
    "\n",
    "Trainer(profiler=AdvancedProfiler())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eae800-2c70-45c7-b2cb-9c7221811fdb",
   "metadata": {},
   "source": [
    "### Early stopping\n",
    "\n",
    "Stop training when a monitored metric has stopped improving (i.e., early stopping)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526cdaa-8572-423b-b0ed-3ac5ec8a7929",
   "metadata": {},
   "source": [
    "#### [TensorFlow (Keras)](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf346a2-8bbd-43b7-b041-c9a7af0dae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tf_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",  # quantity to be monitored\n",
    "    patience=2,  # \"no longer improving\" also means \"for at least 2 epochs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2b439-b6fb-4c0b-85b6-45ee8b97b11d",
   "metadata": {},
   "source": [
    "#### [PyTorch (Lightning)](https://pytorch-lightning.readthedocs.io/en/stable/common/early_stopping.html#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f24e56-0619-42d8-8f18-80273a9599f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_torch_early_stopping = pl.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2800fd-3982-40a9-b715-e06e1bb8aa8c",
   "metadata": {},
   "source": [
    "### Learning rate decay\n",
    "\n",
    "Reduce learning rate when a metric has stopped improving (i.e., reduce the learning rate on plateau)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7fce7f-ea45-4a0b-9126-d27b17078c2c",
   "metadata": {},
   "source": [
    "#### [TensorFlow (Keras)](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef833d8-15b4-4283-96d4-dbe966493750",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tf_learning_rate_decay = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=5,\n",
    "    factor=0.2,  # factor by which the learning rate will be reduced: new_lr = lr * factor\n",
    "    min_lr=0.001,  # lower bound on the learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c53893-1df8-4bc6-8647-b1a112f4c65b",
   "metadata": {},
   "source": [
    "### Example: Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee1ec89-cbac-4b92-bbd7-7928b22f9916",
   "metadata": {},
   "source": [
    "#### TensorFlow (Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cf237-8856-4afd-98f3-9b313d465bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_tf = [\n",
    "    callback_tf_model_checkpoint,\n",
    "    callback_tf_model_backup,\n",
    "    callback_tf_tensorboard_with_profiling,\n",
    "    callback_tf_early_stopping,\n",
    "    callback_tf_learning_rate_decay,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba4177a-8fdf-490d-ac12-24508a288788",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    epochs = 5  # just 1 for the demonstration, would increase this to say 20 normally\n",
    "    model.fit(ds_train, epochs=epochs, validation_data=ds_val, callbacks=callbacks_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10012cb0-785b-41bb-8498-a6db5ff330c7",
   "metadata": {},
   "source": [
    "View the results in TensorBoard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553903e-a953-4cdb-95ac-8f7249ba6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorboard import notebook\n",
    "# notebook.list()\n",
    "# notebook.display(port=6006, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac09ff-e5df-4527-a906-c1e5c126f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir /content/models/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e31054-5f68-4f9f-b1c6-0d5600d4cde8",
   "metadata": {},
   "source": [
    "#### PyTorch (Lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3008f8-d9d1-41cd-a47c-2c6533ca31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks_torch = [\n",
    "#     callback_torch_early_stopping,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07364b36-36b4-4340-8d4a-6f4f7baf0798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer(callbacks=callbacks_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978c8a6-6967-41c4-9f94-6079e1f1d4a5",
   "metadata": {},
   "source": [
    "## Compiling\n",
    "\n",
    "Compile any function in TensorFlow by wrapping it in the [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) decorator.\n",
    "\n",
    "This convert it from eager execution to a static graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560bb9a2-b29c-4213-b81c-64ddb706248f",
   "metadata": {},
   "source": [
    "`torch.jit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436da915-b4af-438c-9e3f-259fa6ef1ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0142d-2014-4bd8-96dc-29271468e750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42a7146e-bc2d-4078-b65e-b6b983d588e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74e521-e75b-4684-a8b3-b01951a40340",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 1\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8af3b-5d9a-4e2c-9bd0-223837749e07",
   "metadata": {},
   "source": [
    "## {ref}`Solutions <models>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85d337-9ea3-4511-ae33-4fb036418318",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de920e-c4c1-4c47-8a80-b7f1ab648754",
   "metadata": {},
   "source": [
    "```{important}\n",
    "\n",
    "- [x] _Tune the models hyperparamaters for the best fit._\n",
    "- [x] _Use transfer learning to save computation on similar problems._\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bcbb5-e4c3-4032-b7af-8fd24d9e19e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Further information\n",
    "\n",
    "### Good practices\n",
    "\n",
    "- See if there is a model architecture (and parameters) that already addresses the task.\n",
    "- Consider the tradeoff between model complexity and size.\n",
    "    - For high accuracy, maybe need a large and complex model.\n",
    "    - For less precision, smaller models use less disk space, memory, and are faster.\n",
    "- Best practices for [PyTorch Lightning model training](https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html)\n",
    "\n",
    "### Other options\n",
    "\n",
    "There are many other options for hyperparameters tuning, including:\n",
    "\n",
    "- [Hyperopt](http://hyperopt.github.io/hyperopt/)\n",
    "    - A general purpose optimisation library.\n",
    "- [scikit-optimise](https://scikit-optimize.github.io/stable/)\n",
    "    - A general purpose optimisation library.\n",
    "- [TPOT](http://epistasislab.github.io/tpot/)\n",
    "    - Automated optimisation library using genetic programming.\n",
    " \n",
    "### Resources\n",
    "\n",
    "#### General\n",
    "\n",
    "- [Model Zoo](https://modelzoo.co/)\n",
    "- [Papers with code - Models](https://paperswithcode.com/methods)\n",
    "- [HuggingFace - Models](https://huggingface.co/models)\n",
    "\n",
    "#### TensorFlow\n",
    "\n",
    "- [TensorFlow Model Garden](https://github.com/tensorflow/models/tree/master/official) for model source code.\n",
    "\n",
    "#### PyTorch\n",
    "\n",
    "- [PyTorch Hub](https://pytorch.org/docs/stable/hub.html) for pre-retrained models.\n",
    "- [Torch Vision Models](https://pytorch.org/vision/stable/models.html)\n",
    "- [Torch Text Models](https://pytorch.org/text/stable/models.html)\n",
    "- [Torch Audio Models](https://pytorch.org/audio/stable/models.html)\n",
    "- [TIMM (pyTorch IMage Models)](https://rwightman.github.io/pytorch-image-models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae56ea9-8437-4cae-9de9-f0b74538d924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1ffad78e3b53a26aeabe29bd69865e9fcde2eed64638bf28084d4e5d53534f3"
  },
  "kernelspec": {
   "display_name": "swd8_intro_ml",
   "language": "python",
   "name": "swd8_intro_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
