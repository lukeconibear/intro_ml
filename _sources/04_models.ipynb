{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f167c0-52a8-4800-9999-0419acc312a8",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae8044-47c9-4c48-b50d-e3dadb441757",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lukeconibear/intro_ml/blob/main/docs/03_models.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49aef6fc-d7fe-45a5-9593-f9092ae02ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you're using colab, then install the required modules\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8934f2-6535-4a79-9f70-7baa5febedaa",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Choosing the best model\n",
    "\n",
    "`keras_tuner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ffaf62-4e70-49fc-8c25-5fdfdcfa4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa113848-d429-4765-9fea-a13b354692d2",
   "metadata": {},
   "source": [
    "https://keras.io/keras_tuner/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f957d13-2a72-42fc-8021-bc74c284678a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f0e60c1-18e5-485c-ab4a-53a91606ad41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f196e67-e94a-45c6-a5b5-0660e5fb27ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c24c725f-0aed-4b26-b2c0-6e02de0ad324",
   "metadata": {},
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59117e95-cf70-4c7e-b667-fe146a31511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 16:25:28.299470: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-14 16:25:28.299487: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea665a2d-75b9-44d7-9cc9-62eb46b82ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, GPUs are not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 16:25:29.209000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-03-14 16:25:29.209027: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-14 16:25:29.209044: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (UOL-LAP-5G6CZH3): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices(\"GPU\"):\n",
    "    print(\n",
    "        f\"Yes, there are {len(tf.config.list_physical_devices('GPU'))} GPUs available.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No, GPUs are not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d1f785-8b2f-4c64-9b55-af983de58c9e",
   "metadata": {},
   "source": [
    "[Keras Applications](https://keras.io/api/applications/)\n",
    "\n",
    "- DenseNet\n",
    "- VGG\n",
    "- EfficientNet\n",
    "- MobileNet\n",
    "- Inception\n",
    "- ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cc9ec15-8e47-4038-8105-5df5229a97be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_v3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInceptionV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclassifier_activation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Instantiates the Inception v3 architecture.\n",
       "\n",
       "Reference:\n",
       "- [Rethinking the Inception Architecture for Computer Vision](\n",
       "    http://arxiv.org/abs/1512.00567) (CVPR 2016)\n",
       "\n",
       "This function returns a Keras image classification model,\n",
       "optionally loaded with weights pre-trained on ImageNet.\n",
       "\n",
       "For image classification use cases, see\n",
       "[this page for detailed examples](\n",
       "  https://keras.io/api/applications/#usage-examples-for-image-classification-models).\n",
       "\n",
       "For transfer learning use cases, make sure to read the\n",
       "[guide to transfer learning & fine-tuning](\n",
       "  https://keras.io/guides/transfer_learning/).\n",
       "\n",
       "Note: each Keras Application expects a specific kind of input preprocessing.\n",
       "For `InceptionV3`, call `tf.keras.applications.inception_v3.preprocess_input`\n",
       "on your inputs before passing them to the model.\n",
       "`inception_v3.preprocess_input` will scale input pixels between -1 and 1.\n",
       "\n",
       "Args:\n",
       "  include_top: Boolean, whether to include the fully-connected\n",
       "    layer at the top, as the last layer of the network. Default to `True`.\n",
       "  weights: One of `None` (random initialization),\n",
       "    `imagenet` (pre-training on ImageNet),\n",
       "    or the path to the weights file to be loaded. Default to `imagenet`.\n",
       "  input_tensor: Optional Keras tensor (i.e. output of `layers.Input()`)\n",
       "    to use as image input for the model. `input_tensor` is useful for sharing\n",
       "    inputs between multiple different networks. Default to None.\n",
       "  input_shape: Optional shape tuple, only to be specified\n",
       "    if `include_top` is False (otherwise the input shape\n",
       "    has to be `(299, 299, 3)` (with `channels_last` data format)\n",
       "    or `(3, 299, 299)` (with `channels_first` data format).\n",
       "    It should have exactly 3 inputs channels,\n",
       "    and width and height should be no smaller than 75.\n",
       "    E.g. `(150, 150, 3)` would be one valid value.\n",
       "    `input_shape` will be ignored if the `input_tensor` is provided.\n",
       "  pooling: Optional pooling mode for feature extraction\n",
       "    when `include_top` is `False`.\n",
       "    - `None` (default) means that the output of the model will be\n",
       "        the 4D tensor output of the last convolutional block.\n",
       "    - `avg` means that global average pooling\n",
       "        will be applied to the output of the\n",
       "        last convolutional block, and thus\n",
       "        the output of the model will be a 2D tensor.\n",
       "    - `max` means that global max pooling will be applied.\n",
       "  classes: optional number of classes to classify images\n",
       "    into, only to be specified if `include_top` is True, and\n",
       "    if no `weights` argument is specified. Default to 1000.\n",
       "  classifier_activation: A `str` or callable. The activation function to use\n",
       "    on the \"top\" layer. Ignored unless `include_top=True`. Set\n",
       "    `classifier_activation=None` to return the logits of the \"top\" layer.\n",
       "    When loading pretrained weights, `classifier_activation` can only\n",
       "    be `None` or `\"softmax\"`.\n",
       "\n",
       "Returns:\n",
       "  A `keras.Model` instance.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/intro_ml/lib/python3.9/site-packages/keras/applications/inception_v3.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.keras.applications.inception_v3.InceptionV3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e195f4c-7d97-4ec8-aac4-285d8bbe0949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7577ded5-464a-4d19-b885-faef0ceb4f4e",
   "metadata": {},
   "source": [
    "https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bbc9af-e5af-41df-ad35-cbb099f6d7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a046d-a96b-40ba-8081-f8113ee54a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3865610-ea8c-45a7-afd8-e19af2d427e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 16:25:29.237337: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58892288/58889256 [==============================] - 5s 0us/step\n",
      "58900480/58889256 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "conv_base = tf.keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\", include_top=False, input_shape=(180, 180, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6fd2952-8c1f-424e-9f97-1a12204ae9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 180, 180, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 180, 180, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 90, 90, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 90, 90, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 90, 90, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 45, 45, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 45, 45, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 45, 45, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 22, 22, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 22, 22, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 11, 11, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeea6a8-9fb4-489d-b4b9-9de83bf322c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc151796-56f6-49bc-891b-fb4d402477a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e2a4917-61bc-4826-9ba5-1deddd5e8d0c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebcfc7-4275-4428-a621-dab215947c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca10882-a972-410b-b556-f14e2363cb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7d4346-f729-49c1-b1dd-734b1a1d1430",
   "metadata": {},
   "source": [
    "## Pretrained\n",
    "\n",
    "...\n",
    "\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49ab731-0a78-4849-b1f6-a9cfcfdd1de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 16:27:12.602805: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-17 16:27:12.602823: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07433409-8583-478d-afc1-5bb44d80f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load a convolutional base with pre-trained weights\n",
    "# base_model = tf.keras.applications.Xception(\n",
    "#     weights='imagenet',\n",
    "#     include_top=False,\n",
    "#     pooling='avg')\n",
    "\n",
    "# # Freeze the base model\n",
    "# base_model.trainable = False\n",
    "\n",
    "# Use a Sequential model to add a trainable classifier on top\n",
    "# model = tf.keras.Sequential(\n",
    "#     [\n",
    "#         base_model,\n",
    "#         tf.keras.layers.Dense(1000),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Compile & train\n",
    "# model.compile(...)\n",
    "# model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4283472d-eb82-482d-a1ba-80ce65f4e23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception (Functional)       (None, 2048)              20861480  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              2049000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22,910,480\n",
      "Trainable params: 2,049,000\n",
      "Non-trainable params: 20,861,480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131c09b-4728-428e-b830-43d33f5cb9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ba8d4cc-6903-42c9-9518-a0dc0dff063b",
   "metadata": {},
   "source": [
    "## [Callbacks](https://keras.io/api/callbacks/)\n",
    "\n",
    "Callbacks are objects that get called by the model at different points during training, in particular after each batch or epoch.\n",
    "\n",
    "For example, they could be used to:\n",
    "\n",
    "- Save a model version at regularly intervals or once attained a metric threshold (i.e., checkpointing).\n",
    "- Monitor and profile the training progress (i.e., TensorBoard)\n",
    "- Change the learning rate when training plateaus.\n",
    "- Fine tuning when the training plateaus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf9462-b57e-4c86-ba1d-2e76c53493a2",
   "metadata": {},
   "source": [
    "### [Checkpointing](https://www.tensorflow.org/guide/checkpoint)\n",
    "\n",
    "For longer or distributed training, it's helpful to save the model (/ model weights) at regular intervals in case it crashes during training (i.e. [model checkpointing](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c472d0-72ec-40d4-a00d-7e6750519d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"models/model_{epoch}\",\n",
    "    save_freq=\"epoch\",  # save a model version at the end of each epoch\n",
    "    save_best_only=True,  # only save a model if val_accuracy improved\n",
    "    monitor=\"val_accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c31d6-aec4-469a-a244-0cc6909cf8e6",
   "metadata": {},
   "source": [
    "### Tensorboard and Profiling\n",
    "\n",
    "[TensorBoard](https://www.tensorflow.org/tensorboard) is a browser-based application that provides live plots of loss and metrics for training and evaluation.\n",
    "\n",
    "You can add [profiling](https://www.tensorflow.org/guide/profiler) too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7d71d4-76a3-4084-9380-bdf14f8ee32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 18:23:38.197379: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-03-17 18:23:38.197406: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-03-17 18:23:38.197650: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "callback_tensorboard_with_profiling = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=\"logs\",\n",
    "    profile_batch=(1, 5),  # profile batches 1 to 5\n",
    "    update_freq=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a2ed4-6869-4303-91ba-cb00d63a24c8",
   "metadata": {},
   "source": [
    "View them with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778c5b07-7cb3-4fbb-b918-68e9f2222237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=/full_path_to_your_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f95ff9-6b9c-474c-9bf8-7a7e1e856509",
   "metadata": {},
   "source": [
    "Also, in-line in [Jupyter Notebooks / Google Colab](https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eae800-2c70-45c7-b2cb-9c7221811fdb",
   "metadata": {},
   "source": [
    "### Metric threshold\n",
    "\n",
    "Stop training when a monitored metric has stopped improving (i.e., [early stopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecf346a2-8bbd-43b7-b041-c9a7af0dae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_accuracy_threshold = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",  # quantity to be monitored\n",
    "    min_delta=1e-2,  # \"no longer improving\" means \"stopped improving by at least 1e-2\"\n",
    "    patience=2,  # \"no longer improving\" also means \"for at least 2 epochs\"\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2800fd-3982-40a9-b715-e06e1bb8aa8c",
   "metadata": {},
   "source": [
    "### Learning rate decay\n",
    "\n",
    "Reduce learning rate when a metric has stopped improving (i.e., [reduce the learning rate on plateau](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef833d8-15b4-4283-96d4-dbe966493750",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_learning_rate_decay = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\",\n",
    "    patience=5,\n",
    "    factor=0.2,  # factor by which the learning rate will be reduced: new_lr = lr * factor\n",
    "    min_lr=0.001,  # lower bound on the learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c53893-1df8-4bc6-8647-b1a112f4c65b",
   "metadata": {},
   "source": [
    "### Example: Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746cf237-8856-4afd-98f3-9b313d465bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    callback_model_checkpoint,\n",
    "    callback_tensorboard_with_profiling,\n",
    "    callback_accuracy_threshold,\n",
    "    callback_learning_rate_decay,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b53c57-4f25-40ac-8f6e-e2f80c3f2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(dataset, epochs=5, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7978c8a6-6967-41c4-9f94-6079e1f1d4a5",
   "metadata": {},
   "source": [
    "## Compiling\n",
    "\n",
    "Compile any function in TensorFlow by wrapping it in the [`@tf.function`](https://www.tensorflow.org/api_docs/python/tf/function) decorator.\n",
    "\n",
    "This convert it from eager execution to a static graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d386a3b-291b-463d-ba38-386a7412e2dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e208ac-f920-4442-9dc0-9b163a21696d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763be35e-6f60-438e-abdd-27e3a2637aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436da915-b4af-438c-9e3f-259fa6ef1ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d0142d-2014-4bd8-96dc-29271468e750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42a7146e-bc2d-4078-b65e-b6b983d588e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74e521-e75b-4684-a8b3-b01951a40340",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 1\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf8af3b-5d9a-4e2c-9bd0-223837749e07",
   "metadata": {},
   "source": [
    "## {ref}`Solutions <models>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba85d337-9ea3-4511-ae33-4fb036418318",
   "metadata": {},
   "source": [
    "## Key Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de920e-c4c1-4c47-8a80-b7f1ab648754",
   "metadata": {},
   "source": [
    "```{important}\n",
    "\n",
    "- [x] _..._\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73bcbb5-e4c3-4032-b7af-8fd24d9e19e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Further information\n",
    "\n",
    "### Good practices\n",
    "\n",
    "- See if there is a model architecture (and parameters) that already addresses the task.\n",
    "\n",
    "\n",
    "### Other options\n",
    "\n",
    "- ...\n",
    " \n",
    "### Resources\n",
    "\n",
    "#### General\n",
    "\n",
    "- [Model Zoo](https://modelzoo.co/)\n",
    "- [Papers with code - Models](https://paperswithcode.com/methods)\n",
    "- [HuggingFace - Models](https://huggingface.co/models)\n",
    "\n",
    "#### TensorFlow\n",
    "\n",
    "- [TensorFlow Hub](https://tfhub.dev/) for pre-trained models.\n",
    "- [TensorFlow Model Garden](https://github.com/tensorflow/models/tree/master/official) for model source code.\n",
    "\n",
    "#### PyTorch\n",
    "\n",
    "- [PyTorch Hub](https://pytorch.org/docs/stable/hub.html) for pre-retrained models.\n",
    "- [Torch Vision Models](https://pytorch.org/vision/stable/models.html)\n",
    "- [Torch Text Models](https://pytorch.org/text/stable/models.html)\n",
    "- [Torch Audio Models](https://pytorch.org/audio/stable/models.html)\n",
    "- [TIMM (pyTorch IMage Models)](https://rwightman.github.io/pytorch-image-models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae56ea9-8437-4cae-9de9-f0b74538d924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1ffad78e3b53a26aeabe29bd69865e9fcde2eed64638bf28084d4e5d53534f3"
  },
  "kernelspec": {
   "display_name": "intro_ml",
   "language": "python",
   "name": "intro_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
